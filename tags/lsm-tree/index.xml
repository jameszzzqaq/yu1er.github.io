<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LSM-Tree on yuler&#39;s blog</title>
    <link>https://zhangyh.me/tags/lsm-tree/</link>
    <description>Recent content in LSM-Tree on yuler&#39;s blog</description>
    <image>
      <url>https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 18 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zhangyh.me/tags/lsm-tree/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reducing DRAM Footprint with NVM in Facebook</title>
      <link>https://zhangyh.me/posts/paper/oct/mynvm/</link>
      <pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/oct/mynvm/</guid>
      <description>BACKGROUND RocksDB and MyRocks RocksDB是基于LSM的kv存储数据库,MyRocks在RocksDB基础上做了MySQL兼容
LinkBench 数据来自于Facebook真实生产环境下知识图谱中的数据.用于访问数据库的操作包括点读取、范围查询（例如通向节点的边列表）、计数查询以及简单的创建、删除和更新操作。
NVM NVM 是一种新的持久存储技术，有可能在数据中心和消费者用例中取代 DRAM. 既可作为字节寻址设备,也可以作为块设备.
INTUITION: THE CHALLENGE OF REPLACING DRAM WITH NVM 按字节计算,NVM比DRAM廉价,但NVM比Flash快一个数量级,因此很有希望作为第二层的存储. 然而NVM作为DRAM的替代品还有一些挑战: 高延迟,低带宽,耐久性,中断延迟
 对于高延迟,可以用大容量NVM替代少容量DRAM作为性能补充(本实验用140G的NVM替换80G的内存),大容量的NVM可以容纳更多的cache,提高命中率. 对于低带宽,DRAM的带宽大于是NVM的35倍左右.我们使用NVM作为块设备,块大小为4kb,因此每次读数据只能以4kb的粒度去读.还有一种写数据的情况,少量的数据跨block存储,这会进一步消耗都带宽.因此只是kv存储的重要限制因素 对于耐久性,DRAM的耐磨损的,而NVM的写入次数是有限的.当NVM替换DRAM作为cache时,这种场景下,频繁的写入逐出会导致NVM的快速磨损.因此需要限制写入的次数. 对于中断延迟,由于相对于其他的块设备,NVM的延迟较低,中断的延迟就变得十分重要.(每次读延迟$10µs$,其中中断占了$2µs$.)  在这些基础上,设计了MyNVM.
Design Satisfying NVM&amp;rsquo;s Read Bandwidth MyNVM默认块大小为16kb,但LinkBench中的数据通常是几十几百字节,因此每次读一个对象,就需要读出16kb的data block,多读了160x左右的数据
从Fig.9可以看出需要两倍的带宽.
Reduce Block Size 直观上来看,减少block size是一个很直接的解决方案. 然而从Fig.9中可以看出从16kb减小到4kb,带宽消耗甚至增加了一倍. 问题就在于data block size减小四倍,就导致了index block size增加了四倍, 从Fig.9就可以看出. 加载到DRAM中的index block占据更多的空间,就导致了DRAM cache命中率降低,
改进方案就是,不再像之前的每个SSTable一个index block,而是使用多个index block,同时为index block创建一个top-level index block.
每次读先查缓存到DRAM中的top-level index block获取对应的index block,然后再把该index block缓存到DRAM中,从而减小了带宽消耗,用额外一次的查询来减少带宽以及内存空间占用.
然而目前带宽消耗还是太高.
Aligning Blocks With Physical Pages 对于$L_1$以及更底层的数据,会进行压缩.</description>
    </item>
    
    <item>
      <title>TRIAD: Creating Synergies Between Memory, Disk and Log in Log Structured Key-Value Stores</title>
      <link>https://zhangyh.me/posts/paper/oct/triad/</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/oct/triad/</guid>
      <description>Introduction 本文通过解决LSM-Tree的写放大问题,从而整体上提高其吞吐量. 在真是的生产环境中,作者观察到compaction最高可占据45%的CPU,平均每天有2.5个小时的时间花在compaction上. 显然,compaction和flush是重要的性能挑战,即使它们发生在面向用户的操作的关键路径之外.
主要面向
Background LSM-Tree:
 Memory Component Disk Component Commit Log  User-facing Operation:
 Get(k) Update(k, v) Delete(k)  Internal Operation:
 Flushing Compaction  Movitation 通过实验将正常的LSM与去掉了Disk Component的LSM相比,发现有3x的吞吐损失. 实验确定了昂贵的 I/O 操作的三个主要来源,每个来源对应于LSM-Tree中三个组件中的一个,即
 data-skew unawareness (memory component) premature and iterative compaction (disk component) duplicated writes (commit log)  data-skew unawareness  hot key被频繁更新,造成了commit log快速增长,但$C_m$变化不大. 在$C_m$达到最大容量之前,频繁的触发更新（为什么？）,在L0中打开和存储文件的固定成本不会因实际向其中写入数据而摊销 data-skew导致$C_d$的多层都会有key的冗余,存储了大量的过时数据  premature and iterative compaction Leveling compaction和Size-Tiered Compaction 每次compaciton选择L0中的一个,但L0层的SSTable的key之间有交叉,即使两个SSTable的key交叉范围很大,仍需要进行两次Compaction,data-skew更是会恶化这种情况.
duplicated writes 当$C_m$刷新到$C_d$之后,需要删除掉其在Commit log中的部分.</description>
    </item>
    
    <item>
      <title>FloDB: Unlocking Memory in Persistent Key-Value Stores</title>
      <link>https://zhangyh.me/posts/paper/sep/flodb/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/sep/flodb/</guid>
      <description>Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化
  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。
  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。
Get 搜索顺序：Membuffer -&amp;gt; Memtable -&amp;gt; SStable
Update Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）
替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。
Scan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。
FloDB Implementation 明确了四各方面:
 Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer -&amp;gt; Hash Table Memtable -&amp;gt; Skiplist
Interaction Between Levels 包括persisting和draining.
Skiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。
批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。
并行性。插入和insert、read并行。
key的临近度决定了path-reuse的效果。</description>
    </item>
    
    <item>
      <title>SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores</title>
      <link>https://zhangyh.me/posts/paper/sep/sfm/</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/sep/sfm/</guid>
      <description>Movitation  传统的merge policy  Level Merge Policy和Tiered Merge Policy在读写放大方面各有优势. 前者相当于单层的Tiered,保证了单层数据的有序性,减少了读放大,但要维护这种有序性,就需要频繁的merge,因此造成了严重的写放大. 后者对有序性的要求并不严格,它只要求每个group的key range互不交叉,允许sstable之间的key交叉.
工作负载的空间局部性  作者对ZippyDB(facebook)的工作负载分析发现,少部分的key被读写多次,大部分的key被读写的次数很少.
Design 作者提出了疑问:
 How to resolve the read amplification problem induced by the tiering merge policy while maintaining its low write amplification?
 根据key-space的不同access pattern,有选择的采用Tiering和Leveling Merge policy. 通过追踪每层中的key-space的访问强度,来决定设置T的值为1还是configured threshold.
Spatially Fragmented LSM-TREE (SFM) 每层通过随机的边界分为不同的key-space.边界的选取方式类似于PebblesDB. 如果一个栈的key-range被认为是读密集的,则设置T为1,否则,按照原有的配置设置.
合并策略如上图
 读超过stack阈值的文件到内存 在内存中进行merge 按照下层的key边界进行切片 新SSTable被刷新到下层 添加到key对应的栈  B. Read Intensity Identification 如何判断读的强度呢? 统计每个段的读写次数.对于读操作,即使读取的key不存在,也是要计入读取次数的. 对于写操作,统计每次merge后的sstable中的kv对的数量. 通过对比每个stack的r/w和整体平均的r/w来判断其为hot还是cold. 根据每个stack的hot还是cold属性,来决定key-space的访问强度.
Metadata Managment 通过StackMetadata的数据结构来管理每个stack的元信息.</description>
    </item>
    
    <item>
      <title>LSM-based storage techniques: a survey</title>
      <link>https://zhangyh.me/posts/paper/aug/suvery-of-lsm/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/aug/suvery-of-lsm/</guid>
      <description> paper原文：LSM-based storage techniques: a survey
 LSM-tree basics 对于索引结构通常有两种更新策略，即in-place update和out-of-place update。
in-place通常使用B+树作为底层数据结构，通过随机写来更新数据项，带来的是读数据的优化，同时每个数据项只存有一份，因此节约了空间。
out-place通常使用lsm tree作为底层数据结构，将随机写转化为顺序写，加快了写的速度，但同时降低了读数据的性能，另外，对同一个key，存有多个版本，带来了空间上的浪费。
LevelDB是谷歌开源的kv数据库，它基于内存-磁盘的存储层次，实现了LSM Tree最基本的功能。 内存中有memtable，使用skiplist来按顺序存储kv对 磁盘中有分层的sstable，使用sorted-string table以文件的形式来存储。 sstable包括了data block、index block、footer block
如今的lsm tree通常为了加快读速度，往往会具有bloom filter组件，该组件有两个操作：插入key和检查key是否存在。为磁盘中的每一level，维护一个bloom filter，从而提高读性能。需要注意，它是false positive的。 同时因为该结构比较小，往往把他缓存到内存当中。
对于failure recovery，使用WAL保证内存中memtable的recovery，使用manifest保证merge过程中sstable的recovery。
有两种常用的merge policy，如上图所示。 Leveling merge policy，要求每一层的SSTable之间不能overlap，merge的时候将第i层的SSTable和第i+1层overlape的所有SSTables进行merge，然后写入i+1层。也就是说通过leveling通过频繁的merge，使得每一层全部有序，牺牲部分写性能来换取读性能。
Tiering merge policy，要求每一层的SSTable之间可以overlap，在sstable达到一定的数量或者size要求后，对该层所有的SSTable进行merge sort，然后直接flush进下一层。该策略通过减少merge操作，牺牲部分读性能换取了写性能。
LSM-tree improvement 文章将对LSM-tree的优化分为以下几个方面：
 Write Amplification: out-of-place数据结构所具有的问题，降低了写性能和磁盘寿命。 Merge Operation: merge后会造成buffer cache miss，大数据量的merge会造成write stall Hardware: 针对large memory,multi-core, SSD/NVM, native storage的优化 Special Workloads: Auto-tuning: 不可能同时达到read,write,space同时最优，并且由于lsm tree拥有大量参数，手动调优也是困难的。 Secondary Indexing: lsm tree只支持简单的kv接口，二级索引也是一个方向。  </description>
    </item>
    
    <item>
      <title>Wisckey: Separating keys from values in ssd-conscious storage</title>
      <link>https://zhangyh.me/posts/paper/aug/wisckey/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/aug/wisckey/</guid>
      <description>WiscKey Design Goals  Low写放大 Low读放大 SSD优化 Feature-rich API (scan、snapshot) Realistic k-v size (value的size通常比key的大很多)  K-V Separation  LSM tree的主要性能成本在于Compaction，但它维护了key的有序性，对于加速读也是十分有必要的。 Compaction只需要对key排序，因此考虑将k-v分开存储。只把value的地址和key放在一起。  LSM tree的size将大大变小，减少了写放大。 读操作虽然需要一次额外的寻址操作，但更小的LSM tree加快了检索同时更容易缓存到大内存中。    Challenge   Parallel Range Query 在levelDB中，范围查询通过iterator的顺序读实现，但是现在由于和key一同存储的只有value的地址，范围查询变为了随机io。 我们在读取所有的地址后，写入一个队列，通过多线程并发读取，来加快范围查询的性能。
  Garbage Collection 由于把所有的数据都存到了vLog里，最朴素的垃圾回收的方法当然是扫描一遍LSM-tree，但开销太大了，只适用于离线环境。 WiscKey在vlog中存储(ksize, vsize, key, value)，并维护head和tail指针。   数据在head处添加 垃圾回收线程从tail处扫描，每次扫描一定量的数据（几MB），然后在LSM-tree中查找，如果kv对有效的话，则再把它添加到head处。 因此，只有tail和head之间的数据是有效的。
Crash Consistency  Optimizations  Value-Log Write Buffer  对于写密集的数据集，频繁的大量的小size的value写入，会导致较大的系统开销。 因此考虑在内存中维护Buffer。 当读数据的时候，先到vlog write buffer中查找，如果没有，再查vlog。 crash-consistency的维护类似于levelDB，使用WAL。
Optimizing the LSM-tree Log  在写LSM-tree之前需要先写vlog，因为这里我们加了vlog write buffer，所以我们直接用vlog的日志作为WAL。</description>
    </item>
    
    <item>
      <title>A Light-weight Compaction Tree to Reduce I/O Amplification toward Efficient Key-Value Stores</title>
      <link>https://zhangyh.me/posts/paper/aug/lwc-tree/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/posts/paper/aug/lwc-tree/</guid>
      <description>A Light-weight Compaction Tree to Reduce I/O Amplification toward Efficient Key-Value Stores Introduction 一般情况下的LSM Tree写放大能达到50X倍 缓解写放大的方法可能有以下几种：
 使用更大的内存buffer 利用设备的特性 逐步散列 KV 项 减少level层级 减少每层的空间放大因子  Design light-weight compaction 也是借鉴了wisckey中提到的，merge and sort过程只是面向key的，不需要value的参与 在对Level i 和 Level i+1进行compaction时，我们把第i层的候选的sstable成为victim，把与victim有overlap的i+1层的sstable成为overlaped. (注意该论文为sstable设计了另外一种存储格式，成为DTable) 因此整个lightweight compaction过程为:
 读取victim到内存中，victim包括了overlaped的metadata 然后根据metadata，将victim分为几个segment 再根据key range将segment追加到对应的DTable中  victim一层的写放大被消除了，随之带来的问题就是DTable的读，虽然每一层的DTable之间是无overlap且有序的，但是DTable内的key值并不是完全有序的。
降低了大概10X的写放大.
Metadata Aggregation overlaped的元数据如何获取。 直觉的方法是直接从overlaped读，但这带来的随机io违背了我们的设计初衷。 另一种是直接在内存中缓存元数据，但这部分开销并不算小，并不划算 为了解决这个问题，我们提出了元数据聚合。 在一次compaction完成后，第i+1层被更新的元数据存储到victim中，以此避免下一次compaction时对metadata的访问呢。
通过compaction过程中一次额外的写，减少AF倍的i+1层的随机读。
Data structure of DTable 每个DTable维护下层的overlaped Dtables metadata 每次compaction过程中append进来的数据作为一个segment,segment之间的key是overlap的,会导致查找性能的下降 MetadaBlock包括了bloom filter blocks,overlaped metada_index block, metada_index block,index block footer</description>
    </item>
    
    <item>
      <title>Redesigning LSMs for nonvolatile memory with NoveLSM</title>
      <link>https://zhangyh.me/topics/paper/novelsm.html</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/novelsm.html</guid>
      <description>Introduction 和目前的存储技术（例如flash、硬盘）相比， NVM有着以下的没有被LSM考虑的优点：
 对于持久存储的随机访问的高性能 in-place update的低成本 (具体是?) 低时延、高带宽为application-level的并行化提供了机会  作者认为探索redesign适用于NVM的LSM是有意义的，而不是design a new data structure。 基于以下考量：
 未来几年NVM和SSD共存，形成异质存储，而不是完全取代。redesign LSM可以在利用NVM的优点的前提下，同时不失去ssd和硬盘最优化的优势。 redesign lsm能为现有的应用提供向后兼容 保证批量写入NVM同样重要（NVM写延迟为DRAM的5-10倍）  Motivation 单纯的硬件NVM的读写是SSD的100倍左右，但在LevelDB在NVM和SSD上的差异只有4-7倍 因此可以说目前的LSM没有充分利用NVM的硬件优势，软件开销较大
Insert Latency insert latency来源有三个方面：
 WAL memtable insert compaction   对于compaction 内存中的mutable memtable在写满之后会刷新成immutable memtable，由后台进程将其压缩进磁盘，同时新开一个mutable memtable，来接管写入 问题是新的mutable memtable写满之后，如果immutable memtable还没有刷到磁盘内，就会造成系统的停顿 当进行大量的写入时，这会成为insert latency的主要来源。
 可能会说，让memtable大一点不就可以解决，但这会带来一系列的问题：
 增大memtable会带来双倍的内存占用，因为mutable memtable和immutable memtable需要同时增大 WAL的磁盘占用也会更大，因为他需要容纳更多的指令 LSM为了性能并不commit log。   Third, LSMs suchas LevelDB and RocksDB do not enforce commits (sync) when writing to a log; as a result, an application crash or power-failure could lead to data loss</description>
    </item>
    
  </channel>
</rss>
