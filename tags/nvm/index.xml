<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NVM on yuler&#39;s blog</title>
    <link>https://blog.zhangyh.me/tags/nvm/</link>
    <description>Recent content in NVM on yuler&#39;s blog</description>
    <image>
      <url>https://blog.zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://blog.zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 18 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.zhangyh.me/tags/nvm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reducing DRAM Footprint with NVM in Facebook</title>
      <link>https://blog.zhangyh.me/posts/paper/21-10/mynvm/</link>
      <pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.zhangyh.me/posts/paper/21-10/mynvm/</guid>
      <description>BACKGROUND RocksDB and MyRocks RocksDB是基于LSM的kv存储数据库,MyRocks在RocksDB基础上做了MySQL兼容
LinkBench 数据来自于Facebook真实生产环境下知识图谱中的数据.用于访问数据库的操作包括点读取、范围查询（例如通向节点的边列表）、计数查询以及简单的创建、删除和更新操作。
NVM NVM 是一种新的持久存储技术，有可能在数据中心和消费者用例中取代 DRAM. 既可作为字节寻址设备,也可以作为块设备.
INTUITION: THE CHALLENGE OF REPLACING DRAM WITH NVM 按字节计算,NVM比DRAM廉价,但NVM比Flash快一个数量级,因此很有希望作为第二层的存储. 然而NVM作为DRAM的替代品还有一些挑战: 高延迟,低带宽,耐久性,中断延迟
 对于高延迟,可以用大容量NVM替代少容量DRAM作为性能补充(本实验用140G的NVM替换80G的内存),大容量的NVM可以容纳更多的cache,提高命中率. 对于低带宽,DRAM的带宽大于是NVM的35倍左右.我们使用NVM作为块设备,块大小为4kb,因此每次读数据只能以4kb的粒度去读.还有一种写数据的情况,少量的数据跨block存储,这会进一步消耗都带宽.因此只是kv存储的重要限制因素 对于耐久性,DRAM的耐磨损的,而NVM的写入次数是有限的.当NVM替换DRAM作为cache时,这种场景下,频繁的写入逐出会导致NVM的快速磨损.因此需要限制写入的次数. 对于中断延迟,由于相对于其他的块设备,NVM的延迟较低,中断的延迟就变得十分重要.(每次读延迟$10µs$,其中中断占了$2µs$.)  在这些基础上,设计了MyNVM.
Design Satisfying NVM&amp;rsquo;s Read Bandwidth MyNVM默认块大小为16kb,但LinkBench中的数据通常是几十几百字节,因此每次读一个对象,就需要读出16kb的data block,多读了160x左右的数据
从Fig.9可以看出需要两倍的带宽.
Reduce Block Size 直观上来看,减少block size是一个很直接的解决方案. 然而从Fig.9中可以看出从16kb减小到4kb,带宽消耗甚至增加了一倍. 问题就在于data block size减小四倍,就导致了index block size增加了四倍, 从Fig.9就可以看出. 加载到DRAM中的index block占据更多的空间,就导致了DRAM cache命中率降低,
改进方案就是,不再像之前的每个SSTable一个index block,而是使用多个index block,同时为index block创建一个top-level index block.
每次读先查缓存到DRAM中的top-level index block获取对应的index block,然后再把该index block缓存到DRAM中,从而减小了带宽消耗,用额外一次的查询来减少带宽以及内存空间占用.
然而目前带宽消耗还是太高.
Aligning Blocks With Physical Pages 对于$L_1$以及更底层的数据,会进行压缩.</description>
    </item>
    
    <item>
      <title>Hashkv: Enabling efficient updates in KV storage via hashing</title>
      <link>https://blog.zhangyh.me/topics/paper/hashkv.html</link>
      <pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.zhangyh.me/topics/paper/hashkv.html</guid>
      <description>Hashkv: Enabling efficient updates in KV storage via hashing Introduction kv分离的Wisckey在vLog的GC上有着以下的问题：
 最新写入的数据被从tail迁移到head处，带来了写放大。 每次GC，对于每一项都需要去查询LSM-Tree，去验证是否还有效，造成大量随机读  Design Storage Management 固定大小的空间单元main segment和log segment。 默认分别为64MB和1MB.
segment group 包括一个main segment和多个log segment.
内存中全局的segment table存储每一个segment group接下来插入或更新的位置。
为了方便GC，segment group存储了key/value size，key，value
GC Collection GC以segment group为单元，当free log segment用尽的时候会触发。
 首先选择候选的segment group，并识别valid KV 然后将valid KV写新的main segment和log segment 再释放之前未使用的log segment 最后更新LSM-Tree中value的最新位置  两个问题：
 如何选择候选的segment group? 根据更新的kv数量，快速选择 如何快速验证KV时valid的 从尾部第一次读到的key，它对应的kv一定是最新的。  Hotness Awareness 在对segment group进行GC后，决定其中数据项为hot还是code。
hot数据仍然写回segment group. code数据写到另外一个区域，旨在segment group中保留其metadata()
如何判断冷热？ 在最后插入后，又更新过一次的即为热数据。
我们称呼该过程为tagging，注意，tagging只发生在GC时。</description>
    </item>
    
    <item>
      <title>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</title>
      <link>https://blog.zhangyh.me/posts/paper/21-8/optane/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.zhangyh.me/posts/paper/21-8/optane/</guid>
      <description>Introduction 过去的很多研究都是基于仿真模拟的NVM，他们基于——NVM表现和DRAM类似，只不过性能相对较低——这样的假设完成了研究。
但实验发现，Optane DIMM的性能与DRAM相比，更依赖于
 access size access method(read,write) pattern degree of concurrency  本文通过经过实验证明了之前的很多仿真方法都是不可靠的。
Background Optane DIMM是第一款商用NVDIMM 与SSD/HDD相比，它low latency、higher read bandwidth、byte-address 与DRAM相比，它higher density、persistent
Optane Memory Optane DIMM使用和DRAM相同的插槽，和处理器的集成内存控制器（iMC)相连接。Intel Cascade Lake处理器是第一个支持Optane DIMM的处理器，有一个或两个processor die，每个die支持两个iMC，每个iMC支持三个channel，因此一个die可以支持6个Optane DIMM
为了持久化，iMC维护一个asynchronous DRAM refresh(ADR)区域，写入这个区域内的数据都能保证其被持久化。ADR区域不保证处理器cache的持久化。
iMC按照缓存行粒度(64byte)和Optane DIMM通信。
Optane DIMM中
3D-XPoint物理介质的访问粒度为256byte，因此会造成写放大，所以会在XPController中有controller负责将小于256B的操作变成256B的访问，同时内部有Buffer用来合并临近的访问。
为了磨损均匀、坏块的管理，AIT用于进行内部地址转换。
Operation Model Optane DIMM有两种模式Memory和App Direct
 Memory Model 该模式下直接将Optane DIMM作为普通的内存使用，将DRAM作为内存的Cache，DRAM是透明的，直接观察到的内存容量就是Optane DIMM的容量。 该模式可以解决一些内存数据库内存容量不足的问题。 App Direct Model 该模式将Optane DIMM作为持久化设备使用，直接通过CPU指令读写。文件系统和其他的管理层管理持久化内存的分配、回收和访问。  支持交错访问，最小的单位是4KB，保证一个page一定是从一个DIMM中读取出来的
ISA Support store: 绕过store buffer ntstore: 绕过CPU cache，直接写到内存。一般用于写完就不管的情况，可以防止污染cache。 clflush: 把cache line刷回内存，并且让cache line失效。只能串行执行。 clflushopt: 功能同clflush，但是不同缓存行可以并发执行。 clwb: 除了写回后不让cache line失效，其他同clflushopt。 sfence：写屏障，在sfence指令前的写操作当必须在sfence指令后的写操作前完成</description>
    </item>
    
    <item>
      <title>Redesigning LSMs for nonvolatile memory with NoveLSM</title>
      <link>https://blog.zhangyh.me/topics/paper/novelsm.html</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.zhangyh.me/topics/paper/novelsm.html</guid>
      <description>Introduction 和目前的存储技术（例如flash、硬盘）相比， NVM有着以下的没有被LSM考虑的优点：
 对于持久存储的随机访问的高性能 in-place update的低成本 (具体是?) 低时延、高带宽为application-level的并行化提供了机会  作者认为探索redesign适用于NVM的LSM是有意义的，而不是design a new data structure。 基于以下考量：
 未来几年NVM和SSD共存，形成异质存储，而不是完全取代。redesign LSM可以在利用NVM的优点的前提下，同时不失去ssd和硬盘最优化的优势。 redesign lsm能为现有的应用提供向后兼容 保证批量写入NVM同样重要（NVM写延迟为DRAM的5-10倍）  Motivation 单纯的硬件NVM的读写是SSD的100倍左右，但在LevelDB在NVM和SSD上的差异只有4-7倍 因此可以说目前的LSM没有充分利用NVM的硬件优势，软件开销较大
Insert Latency insert latency来源有三个方面：
 WAL memtable insert compaction   对于compaction 内存中的mutable memtable在写满之后会刷新成immutable memtable，由后台进程将其压缩进磁盘，同时新开一个mutable memtable，来接管写入 问题是新的mutable memtable写满之后，如果immutable memtable还没有刷到磁盘内，就会造成系统的停顿 当进行大量的写入时，这会成为insert latency的主要来源。
 可能会说，让memtable大一点不就可以解决，但这会带来一系列的问题：
 增大memtable会带来双倍的内存占用，因为mutable memtable和immutable memtable需要同时增大 WAL的磁盘占用也会更大，因为他需要容纳更多的指令 LSM为了性能并不commit log。   Third, LSMs suchas LevelDB and RocksDB do not enforce commits (sync) when writing to a log; as a result, an application crash or power-failure could lead to data loss</description>
    </item>
    
    <item>
      <title>Zen: a high-throughput log-free OLTP engine for non-volatile main memory</title>
      <link>https://blog.zhangyh.me/posts/paper/21-8/zen/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.zhangyh.me/posts/paper/21-8/zen/</guid>
      <description>Design Overview  Hybrid Table(HTable)  tuple heap in NVM Met-Cache in DRAM per-thread NVM-tuple managers.   Metadata in NVM Transaction and Indices in DRAM  NVM heap 所有的tuple持久化到NVM中的tuple heap，它由大小为2MB的pages组成 其中可能同时存在某个数据的多个版本 (tupleId, Tx-CTS)唯一确定一个tuple
 LP: 在commited transaction中最后一个被持久化的tuple，将该标记记为1 Tx-CTS: 每个事务在线程内都有唯一ID，单调递增。 Deleted: 被删除  Met-Cache NVM heap在DRAM中的缓存
 Clock bit: 用于Clock置换策略 Active bit: 事务正在使用该tuple Dirty bit: 被修改，当事务中途被absort，根据dirty位重新到NVM Heap获取tuple Copy bit: tuple被复制 CC-Meta：用于DRAM中的并发控制时，存储对应的信息。Zen中的并发控制完全实现在DRAM中，支持多种并发控制策略。  Indices in DRAM 在DRAM维护索引。crash后会重建。索引指向Met-Cache或者NVM heap中的tuple
Transaction-Private Data 线程私有 用于事务并发访问 存储相应数据</description>
    </item>
    
  </channel>
</rss>
