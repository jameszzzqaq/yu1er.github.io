<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | yuler's blog</title><meta name=keywords content><meta name=description content="Posts - yuler's blog"><meta name=author content="yuler"><link rel=canonical href=https://blog.zhangyh.me/posts/><meta name=google-site-verification content="G-JP3WQ36T5K"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabcsadf"><link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style><link rel=icon href=https://blog.zhangyh.me/icon/jiaran16.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.zhangyh.me/icon/jiaran16.ico><link rel=icon type=image/png sizes=32x32 href=https://blog.zhangyh.me/icon/jiaran32.ico><link rel=apple-touch-icon href=https://blog.zhangyh.me/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://blog.zhangyh.me/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.99.1"><link rel=alternate type=application/rss+xml href=https://blog.zhangyh.me/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JP3WQ36T5K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JP3WQ36T5K",{anonymize_ip:!1})}</script><meta property="og:title" content="Posts"><meta property="og:description" content="yuler's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://blog.zhangyh.me/posts/"><meta property="og:image" content="https://blog.zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="yuler's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="yuler's blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.zhangyh.me/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.zhangyh.me/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://blog.zhangyh.me/archives/ title=archives><span>archives</span></a></li><li><a href=https://blog.zhangyh.me/categories/ title=categories><span>categories</span></a></li><li><a href=https://blog.zhangyh.me/tags/ title=tags><span>tags</span></a></li><li><a href=https://blog.zhangyh.me/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://blog.zhangyh.me/>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores</h2></header><section class=entry-content><p>Movitation 传统的merge policy Level Merge Policy和Tiered Merge Policy在读写放大方面各有优势. 前者相当于单层的Tiered,保证了单层数据的有序性,减少了读放大,但要维护这种有序性,就需要频繁的merge,因此造成了严重的写放大. 后者对有序性的要求并不严格,它只要求每个group的key range互不交叉,允许sstable之间的key交叉.
工作负载的空间局部性 作者对ZippyDB(facebook)的工作负载分析发现,少部分的key被读写多次,大部分的key被读写的次数很少.
Design 作者提出了疑问:
How to resolve the read amplification problem induced by the tiering merge policy while maintaining its low write amplification?
根据key-space的不同access pattern,有选择的采用Tiering和Leveling Merge policy. 通过追踪每层中的key-space的访问强度,来决定设置T的值为1还是configured threshold.
Spatially Fragmented LSM-TREE (SFM) 每层通过随机的边界分为不同的key-space.边界的选取方式类似于PebblesDB. 如果一个栈的key-range被认为是读密集的,则设置T为1,否则,按照原有的配置设置.
合并策略如上图
读超过stack阈值的文件到内存 在内存中进行merge 按照下层的key边界进行切片 新SSTable被刷新到下层 添加到key对应的栈 B. Read Intensity Identification 如何判断读的强度呢? 统计每个段的读写次数.对于读操作,即使读取的key不存在,也是要计入读取次数的. 对于写操作,统计每次merge后的sstable中的kv对的数量. 通过对比每个stack的r/w和整体平均的r/w来判断其为hot还是cold. 根据每个stack的hot还是cold属性,来决定key-space的访问强度.
Metadata Managment 通过StackMetadata的数据结构来管理每个stack的元信息....</p></section><footer class=entry-footer>9-24&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores" href=https://blog.zhangyh.me/posts/paper/21-9/sfm/></a></article><article class=post-entry><header class=entry-header><h2>SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores</h2></header><section class=entry-content><p>本文旨在解决LSM-Tree的长尾问题，长尾问题的根源主要是客户端操作和LSM-Tree内部操作的互相干扰。 客户端操作：write 内部操作: flush、compaction
Experiment study of tail latency 实验一：对比RocksDB和RocksDB without flushing. RocksDB without flushing: 要flush的immutable memtable直接丢弃 实验二：Rate-limited RocksDB 实验三：RocksDB with increased Memtable 实验四：TRAID 实验五：PebblesDB 作者对目前最新的LSM-Tree数据库RocksDB、TRIAD、PebblesDB进行了实验并得出三点结论。
长尾的主要原因是被写满的memtable阻塞了write。 有两个原因导致了这种情况：第一，磁盘上的L0-L1 compaction跟不上写入的速度，导致L0被写满。第二，意外的有大量的compaction在同时进行，占据了大量的IO，导致flush因为有限的带宽而变慢，使得memtable变满。 简单的限制内部操作带宽, 并不能解决flush带宽受限的问题，长远来看反而会加剧这种问题。这种方法能够推迟压缩，但增加了在未来某个时候同时发生compaction的可能性。 提高吞吐量的方法，例如选择性地启动压缩或仅在最高level执行压缩，在短期内避免了延迟峰值，但从长远来看会加剧问题，因为它们也会增加在稍后的某个时间点进行许多并发压缩的可能性。 推论： 由1得出推论：内部操作并不是完全平等的。更low-level的操作是关键的，因为未能及时完成它们可能会导致客户端操作停滞。 由2，3得出推论：必须进行长时间的测试，避免问题未被发现。 (因为写入的数据量不够的话，无法体现更底层的compaction操作对系统的影响。)
SILK Design principles 有选择的分配带宽。 遇到峰值流量时，分配更多的带宽给low-level，减缓high-level的压缩。 在闲时，利用短暂的低负载时期来促进内部操作的处理 优先处理更low-level的操作 给内部操作指定优先级: flush > L0-L1 compaction > high-level compaction 抢占式compaction 允许更low-level的compaction抢占high-level的compaction Implementation 有选择的分配带宽 有优先次序和可抢占的内部操作 系统内部维护两个线程池：高优先级池用于flush,低优先级池用于compaction. Flush: 有专门的线程池，在mem被写满之前，提供稳定的一定量的带宽，以保证不断的写入。多个内存组件和多个线程可能保证较长的性能高峰。 L0-L1 compaction: 和high-level的compaction共用线程，该过程需要保证L0有足够的空间供flush。如果需要进行L0-L1的compaction，并且目前没有可用线程，那么就会抢占high-leve compaction的线程。 high-level compaction: 线程维护在低优先级线程池</p></section><footer class=entry-footer>9-22&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores " href=https://blog.zhangyh.me/posts/paper/21-9/silk/></a></article><article class=post-entry><header class=entry-header><h2>weekly-contest-259</h2></header><section class=entry-content><p>第259次周赛 A. 执行操作后的变量值 时间复杂度: $O(n)$ 空间复杂度: $O(1)$ class Solution { public: int finalValueAfterOperations(vector&lt;string>& operations) { int X = 0; for (auto & s: operations) { if (s[1] == '+') X++; else X--; } return X; } }; B. 数组美丽值求和 通过正反两次遍历记录每个值左边的最大值和右边的最小值.
时间复杂度: $O(n)$ 空间复杂度: $O(n)$ class Solution { public: int sumOfBeauties(vector&lt;int>& nums) { int n = nums.size(); vector&lt;int> left(n, INT_MIN), right(n, INT_MAX); int ans = 0; left[0] = nums[0]; right[n - 1] = nums[n - 1]; for (int i = 1; i &lt; n; i++) left[i] = max(left[i-1], nums[i]); for (int i = n - 2; i >= 0; i--) right[i] = min(right[i + 1], nums[i]); for (int i = 1; i &lt; n - 1; i++) { if (left[i - 1] &lt; nums[i] && nums[i] &lt; right[i + 1]) ans +=2; else if (nums[i - 1] &lt; nums[i] && nums[i] &lt; nums[i + 1]) ans++; } return ans; } }; C....</p></section><footer class=entry-footer>9-20&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to weekly-contest-259" href=https://blog.zhangyh.me/posts/leetcode/weekly-contest-259/></a></article><article class=post-entry><header class=entry-header><h2>biweekly-contest-61</h2></header><section class=entry-content><p>第61次双周赛 A: 差的绝对值为 K 的数对数目 时间复杂度:$O(n)$ 空间复杂度:$O(n)$ class Solution { public: int countKDifference(vector&lt;int>& nums, int k) { map&lt;int, int> m; int ans; for (auto && x : nums) { ans += (m[x + k] + m[x - k]); m[x]++; } return ans; } }; B: 从双倍数组中还原原数组 两个关键点: 1. 元素个数一定为偶数 2. 结果数组元素个数一定是原数组二分之一
时间复杂度: 排序: $O(nlgn)$ 遍历: $O(n)$ 空间复杂度: $O(n)$ class Solution { public: vector&lt;int> findOriginalArray(vector&lt;int>& changed) { auto n = changed....</p></section><footer class=entry-footer>9-19&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to biweekly-contest-61" href=https://blog.zhangyh.me/posts/leetcode/biweekly-contest-61/></a></article><article class=post-entry><header class=entry-header><h2>The Google File System</h2></header><section class=entry-content><p>1 Introduction 本文是《The Google File System》的学习笔记
2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。
廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要 2.2 Interface 未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append 2.3 Architecture 集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata 2....</p></section><footer class=entry-footer>8-28&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to The Google File System" href=https://blog.zhangyh.me/posts/paper/21-8/gfs/></a></article><article class=post-entry><header class=entry-header><h2>聊聊内存对齐</h2></header><section class=entry-content><p>为什么需要内存对齐？ 首先需要从物理硬件上了解计算机如何进行内存访问的。
Channel > DIMM > Rank > Chip > Bank > Coloum/Row > Cell
如上图 CPU包括两个Channel 每个Channel包括两个DIMM 每个DIMM由Rank组成 Rank由8个内存颗粒chip组成 每个Chip包括8个Bank
CPU读取内存时从8个chip中每个读取8bit字节，从而构成64bit column和row定位的一个单元格cell中有8个bit
因此一次性最少读取64bit，这恰好也是cacheline的大小。 （cacheline是cache的基本单位，每个cache由若干cacheline组成）
什么是内存对齐 从上面我们可以知道，一次性最少读取8B，这也是局部性原理的一种使用。
内存对齐就是代码编译后在内存中的布局，当一个内存地址刚好能够整除8，就称为其内存地址是8字节对齐的。
为什么需要内存对齐 对于go中定义的如下结构
type Type1 struct { a int8 b int64 c int32 } 其内存布局是这样的： 通过unsafe.Sizeof()可以打印出该结构占用了24字节的内存。
如果调整一下顺序：
type Type2 struct { a int8 c int32 b int64 } 便可以节约一个字节的内存
零大小字段 如果结构体或数组类型不包含大小大于零的字段或元素，那么它的大小就为0
package main import ( "fmt" "unsafe" ) type M struct { m int64 x struct{} } type N struct { x struct{} n int64 } func main() { m, n := M{}, N{} fmt....</p></section><footer class=entry-footer>8-13&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to 聊聊内存对齐" href=https://blog.zhangyh.me/posts/misc/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/></a></article><article class=post-entry><header class=entry-header><h2>LSM-based storage techniques: a survey</h2></header><section class=entry-content><p>paper原文：LSM-based storage techniques: a survey
LSM-tree basics 对于索引结构通常有两种更新策略，即in-place update和out-of-place update。
in-place通常使用B+树作为底层数据结构，通过随机写来更新数据项，带来的是读数据的优化，同时每个数据项只存有一份，因此节约了空间。
out-place通常使用lsm tree作为底层数据结构，将随机写转化为顺序写，加快了写的速度，但同时降低了读数据的性能，另外，对同一个key，存有多个版本，带来了空间上的浪费。
LevelDB是谷歌开源的kv数据库，它基于内存-磁盘的存储层次，实现了LSM Tree最基本的功能。 内存中有memtable，使用skiplist来按顺序存储kv对 磁盘中有分层的sstable，使用sorted-string table以文件的形式来存储。 sstable包括了data block、index block、footer block
如今的lsm tree通常为了加快读速度，往往会具有bloom filter组件，该组件有两个操作：插入key和检查key是否存在。为磁盘中的每一level，维护一个bloom filter，从而提高读性能。需要注意，它是false positive的。 同时因为该结构比较小，往往把他缓存到内存当中。
对于failure recovery，使用WAL保证内存中memtable的recovery，使用manifest保证merge过程中sstable的recovery。
有两种常用的merge policy，如上图所示。 Leveling merge policy，要求每一层的SSTable之间不能overlap，merge的时候将第i层的SSTable和第i+1层overlape的所有SSTables进行merge，然后写入i+1层。也就是说通过leveling通过频繁的merge，使得每一层全部有序，牺牲部分写性能来换取读性能。
Tiering merge policy，要求每一层的SSTable之间可以overlap，在sstable达到一定的数量或者size要求后，对该层所有的SSTable进行merge sort，然后直接flush进下一层。该策略通过减少merge操作，牺牲部分读性能换取了写性能。
LSM-tree improvement 文章将对LSM-tree的优化分为以下几个方面：
Write Amplification: out-of-place数据结构所具有的问题，降低了写性能和磁盘寿命。 Merge Operation: merge后会造成buffer cache miss，大数据量的merge会造成write stall Hardware: 针对large memory,multi-core, SSD/NVM, native storage的优化 Special Workloads: Auto-tuning: 不可能同时达到read,write,space同时最优，并且由于lsm tree拥有大量参数，手动调优也是困难的。 Secondary Indexing: lsm tree只支持简单的kv接口，二级索引也是一个方向。</p></section><footer class=entry-footer>8-10&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to LSM-based storage techniques: a survey" href=https://blog.zhangyh.me/posts/paper/21-8/suvery-of-lsm/></a></article><article class=post-entry><header class=entry-header><h2>Wisckey: Separating keys from values in ssd-conscious storage</h2></header><section class=entry-content><p>WiscKey Design Goals Low写放大 Low读放大 SSD优化 Feature-rich API (scan、snapshot) Realistic k-v size (value的size通常比key的大很多) K-V Separation LSM tree的主要性能成本在于Compaction，但它维护了key的有序性，对于加速读也是十分有必要的。 Compaction只需要对key排序，因此考虑将k-v分开存储。只把value的地址和key放在一起。 LSM tree的size将大大变小，减少了写放大。 读操作虽然需要一次额外的寻址操作，但更小的LSM tree加快了检索同时更容易缓存到大内存中。 Challenge Parallel Range Query 在levelDB中，范围查询通过iterator的顺序读实现，但是现在由于和key一同存储的只有value的地址，范围查询变为了随机io。 我们在读取所有的地址后，写入一个队列，通过多线程并发读取，来加快范围查询的性能。
Garbage Collection 由于把所有的数据都存到了vLog里，最朴素的垃圾回收的方法当然是扫描一遍LSM-tree，但开销太大了，只适用于离线环境。 WiscKey在vlog中存储(ksize, vsize, key, value)，并维护head和tail指针。 数据在head处添加 垃圾回收线程从tail处扫描，每次扫描一定量的数据（几MB），然后在LSM-tree中查找，如果kv对有效的话，则再把它添加到head处。 因此，只有tail和head之间的数据是有效的。
Crash Consistency Optimizations Value-Log Write Buffer 对于写密集的数据集，频繁的大量的小size的value写入，会导致较大的系统开销。 因此考虑在内存中维护Buffer。 当读数据的时候，先到vlog write buffer中查找，如果没有，再查vlog。 crash-consistency的维护类似于levelDB，使用WAL。
Optimizing the LSM-tree Log 在写LSM-tree之前需要先写vlog，因为这里我们加了vlog write buffer，所以我们直接用vlog的日志作为WAL。</p></section><footer class=entry-footer>8-10&nbsp;·&nbsp;yuler</footer><a class=entry-link aria-label="post link to Wisckey: Separating keys from values in ssd-conscious storage" href=https://blog.zhangyh.me/posts/paper/21-8/wisckey/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://blog.zhangyh.me/posts/>« Prev Page</a>
<a class=next href=https://blog.zhangyh.me/posts/page/3/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://blog.zhangyh.me/>yuler's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>