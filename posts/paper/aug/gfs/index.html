<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>The Google File System | yuler's blog</title>
<meta name=keywords content>
<meta name=description content="1 Introduction 本文是《The Google File System》的学习笔记
2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。
 廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要  2.2 Interface  未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append  2.3 Architecture  集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata  2.">
<meta name=author content="yuler">
<link rel=canonical href=https://zhangyh.me/posts/paper/aug/gfs/>
<meta name=google-site-verification content="G-JP3WQ36T5K">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabcsadf">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<link rel=icon href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=16x16 href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=32x32 href=https://zhangyh.me/icon/jiaran32.ico>
<link rel=apple-touch-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JP3WQ36T5K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JP3WQ36T5K',{anonymize_ip:!1})}</script>
<meta property="og:title" content="The Google File System">
<meta property="og:description" content="1 Introduction 本文是《The Google File System》的学习笔记
2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。
 廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要  2.2 Interface  未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append  2.3 Architecture  集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata  2.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://zhangyh.me/posts/paper/aug/gfs/"><meta property="og:image" content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-28T00:00:00+00:00">
<meta property="article:modified_time" content="2021-08-28T00:00:00+00:00"><meta property="og:site_name" content="yuler's blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="The Google File System">
<meta name=twitter:description content="1 Introduction 本文是《The Google File System》的学习笔记
2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。
 廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要  2.2 Interface  未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append  2.3 Architecture  集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata  2.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zhangyh.me/posts/"},{"@type":"ListItem","position":2,"name":"The Google File System","item":"https://zhangyh.me/posts/paper/aug/gfs/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Google File System","name":"The Google File System","description":"1 Introduction 本文是《The Google File System》的学习笔记\n2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。\n 廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要  2.2 Interface  未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append  2.3 Architecture  集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata  2.","keywords":[""],"articleBody":"1 Introduction 本文是《The Google File System》的学习笔记\n2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。\n 廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要  2.2 Interface  未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append  2.3 Architecture  集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata  2.4 Single Master  单 mater 简化了设计，但有成为 bottleneck 的风险 client 会缓存 metadata，从而减少和 master 的通信  2.5 Chunk Size chunk size is one of the key design parameters 大 chunk size 的advantages\n 减少 client 和 master 之间对于 metadata 通信 client 对 chunkserver 的某一 chunk 进行操作时，通过 tcp 持久连接减少网络负载 减少了 metadata 在 master 存储的大小 大 chunk size 的 disadvantages 对于小文件产生的 chunk 少，多个 client 的时可能会形成 hot spot。 但目前来看不是主要问题  2.6 Metadata 有三种主要类型的 metadata\n file and namespace file - chunk handle chunk handle - locations of each replica   metadata 全部存储在 master 的内存中 1和2会以 operation log 的形式持久化到磁盘和远端机器中 3不持久化到磁盘中，master 启动的时候会和所有的 chunkserver 通信来获取3，之后在运行中通过 heartbeat 来维护  2.6.1 In-Memory Data Structures 全内存有利于进行快速的 scan，通过 scan 我们能够完成以下操作。\n chunk garbage collection (参考section 4.4.2) chunkerserver 故障时的 chunk 再复制 在 chunkserver 间的 chunk 迁移，用来平衡负载和磁盘使用率  全内存导致chunk的数量受限于 master 内存的大小，但作者有以下的考量\n 64 bit 即可管理一个 64MB 的 chunk 大部分 chunk 都是满的 ( full ) 单纯增加内存的开销比较低 (与增加整个系统的复杂度相比)  2.6.2 Chunk Locations chunk location 信息由 chunkserver 来维护，master 通过 hearbeat 通信来获取，并存放到 metadata 中\n但 chunk locations 并不永久存放在 master 中，参考 section 2.6\n简化了 chunkserver 加入、退出、更名、故障、重启的时候，master 与 chunkserver 同步的问题\n2.6.3 Operation Log  日志记录了重要的 metadata 的变动, 被持久化到本地和多台远程机器上 只有当日志持久化成功时，才会认为整个操作完成，返回成功给客户端 为了加快 master 的启动，会设置 checkpoint，checkpoint 以 b树 存储，可以直接映射到 master 内存中 master 通过最新的 checkpoint 和 checkpoint 之后的日志来进行数据恢复  2.7 Consistency Model GFS采用较宽松的一致性模型\n2.7.1 Guarantee by GFS  文件命名空间的改动是原子的 namespace lock 保证了原子性和正确性 section 4.1  一致性模型的定义\n定义：\n consistent：如果所有客户端无论从哪个副本读，读到的数据都是一样的，那么就认为这个数据是一致的。 defined：修改文件之后，相关数据是一致的，并且客户端能够读取到它刚才修改的内容，那么相关数据是已定义的。已定义暗含了一致。  GFS 提供一个弱一致性模型。对于并发随机写入操作来说，数据是一致的，但是未定义的。对于追加写入操作来说，客户端总是可以读取到一致并且已定义的数据，但是实际储存在各个 chunk 服务器上的数据可能有部分是不一致的。\n2.7.2 Implications of Applications GFS 通过以下几点保证数据一致性：\n 对 chunk 的所有副本修改操作顺序一致。 使用 chunk 版本号来检测副本因为服务器宕机而失效。 与服务器定期握手来找到失效的 chunk 服务器。 使用 checksum 来校验数据是否损坏。  3 System Interactions 原则：最小化所有操作与master的交互\n3.1 Leases and Mutation Order  多个 replica 中只有一个获得 lease，因此只有一个 primary，其余为 secondary 由 primary 决定 chunk 变更的顺序，secondary 按照 primary 的要求，按照指定顺序进行变更 lease 默认有 60s 的租期，同一时刻只有一台 primary 持有 lease\n  写入流程\n client 向 master 询问持有 chunk 租约的 chunkserver 以及 chunk 其它副本的位置。 如果没有，master 选择chunk 的一个副本建立租约 master 将主 chunkserver 的标识和其它副本的位置返回给 client，client 进行缓存 client 将数据推送到所有副本。chunkserver 将数据保存在内部的 LRU 缓存，此时并没有写入磁盘 一旦所有副本回复已经收到数据，client 向 primary 发送写请求。primary 向接收到的所有操作分配连续的序列号，操作可能来自不同 client primary 把写请求发送给所有 secondary，secondary 按照主副本分配的序列号以相同的顺序执行 secondary 向主副本回复完成操作 primary 回复 client。任何副本的 error 都会返回，client 重新执行 3-7 的操作  3.2 Data Flow  data flow 与 control flow 分离，提高网络效率 使用 chain 式的发送，而不是 tree 式，从而充分利用每台机器的带宽，避免瓶颈 尽量选择没有接受数据且离自己最近的机器发送数据，通过 ip address 来估计机器之间的距离 采用 tcp 来发送数据  3.3 Atomic Record Appends  GFS 保证追加记录的原子性。当存在多个并发的追加记录时，GFS 对于每个追加记录至少有一次是写入成功的。GFS 指定写入的 offset 并且在之后返回，所有副本中的 offset 是一致的。 追加记录在之前描述的控制流程之上加了一些额外的步骤。对于一次指定的追加记录来说，主 chunk 服务器会检查给定 chunk 的大小。如果追加记录使得这个 chunk 的大小超过限制，服务器会填充这个 chunk 到最大大小然后指示客户端发起请求写入下一个 chunk。 如果追加记录在任意一个 chunk 服务器失败了，客户端需要进行重新操作。重新操作的结果是，同一个 chunk 的副本可能拥有不一致的记录。GFS 不保证所有副本在字节序上是完全一致的，但至少保证有一次成功的写入。 因此，追加记录将使数据是一致的，但可能包含部分不一致的局部数据，如填充和错误。  3.4 Snapshot copy-on-write\n snapshot 前，首先收回相关 chunk 的 lease，这保证了需要修改 chunk 时，首先要和 master 交互，从而在修改之前进行复制 master 维持一个对所有 chunk 的引用计数。当客户端发起一次快照请求时，并不实际进行复制，而是将相应引用计数加一。当对相应服务器发起写入请求时，master 注意到它的引用计数大于一，然后才复制一个新的 chunk，并指示客户端写入这个新 chunk。引用计数减一  4. Master Operation 4.1 Namespace Management and Locking  GFS 的名称空间是一个全局路径和元数据映射关系的查找表。这个表使用前缀压缩储存在内存中。在名称空间的属性结构上，每个节点都是都有一个关联的读写锁。 每一个master操作在执行前，都需要获取锁。假设我们要操作 /d1/d2/…/dn/leaf,需要先获取 /d1, /d1/d2, …,/d1/d2/…/dn的读锁，在获取 /d1/d2/…/dn/leaf 的写锁或读锁  4.2 Replica Placement 副本放置的两个主要原则：\n 最大化数据的可靠性和可用性 最大化网络带宽的利用率  4.3 Creation, Re-replication, Rebalancing chunk被创建通常处于三种目的creation, re-replication, rebalancing\nchunk creation 存放位置, 按优先级考虑以下三种因素：\n chunkserver 的磁盘使用率（与平均水平相比） chunkserver 上最近创建 chunk 的数量 chunkserver 的地理位置分布（是否在一个 rack 上）  当副本数量低于用户指定的水平是，会进行副本的 re-replication\n优先考虑以下的 chunk 进行 re-replication\n 优先复制总数量更少的 chunk 优先复制活跃的 chunk 优先复制阻塞 client 流程的 chunk  会定期进行 chunk 的 rebalancing，是每台 chunkserver 都拥有大致相同的磁盘使用水平，以充分利用磁盘\n4.4 Garbage Collection 惰性的垃圾回收，使得整个系统更为简单\n4.4.1 Mechanism 文件删除流程\n 记录删除操作的 log 将文件 rename 成为一个包含时间戳信息的隐藏的名字 master 对 metadata 进行 scan 时，删除3天前被删除文件的元数据 （也就是三天内可以对文件进行恢复，这个时间是可以配置的） master 通过与 chunkserver 通信，彻底删除磁盘上的文件  4.4.2 Discussion GFS的垃圾回收较为容易，通过 file-arrays of chunk handles 的映射找出所有未被使用的 chunk， 通过 delay delete 的方式删除（即4.4.1所说）\nadvantages:\n 在大规模系统中简单可靠， 多个删除操作在 master 较为空闲时进行后台批操作，分散开销 delay delete 为错误删除提供了安全保障  disadvantages:\n delay delete 在磁盘空间紧张时阻碍了用户的使用 对于重复进行 create 和 delete 的应用不能马上释放资源（GFS可以配置指定目录下的文件删除策略——如即时删除、无副本）  4.5 Stale Replica Detection  每一个chunk，在master中都维护了一个 version number，同时 chunkserver 中也记录了当前的 version master 与 chunk 签订租约时增加版本号，然后通知副本( Replica ) master 在垃圾回收的过程中移除过期失效的版本，在平时的读写操作中不会考虑过期版本  5. Fault Tolerance and diagnosis 最大挑战之一：频繁的组件故障\n5.1 High Availablity 保证高可用的两个策略：fast recovery 和 replication.\n5.1.1 Fast Recovery  数秒内恢复 不区分异常关闭和正常关闭  5.1.2 Chunk Replication  可以为不同的 namespace 设置不同的复制等级，默认为3 奇偶校验和纠删码 其他的冗余方案  5.1.3 Master Replication  master 所有操作日志和 checkpoint 备份在多个机器上 每个操作只有在写入磁盘和多个 master 备份后，才算生效 master 启动备用机器，通过 dns 别名的方式更改 client 指向为对 master 备用机的访问 影子 master 提供只读服务，通常由几秒分之一的滞后，通常体现在 metadata 上  5.2 Data Integrity  chunkserver 使用 checksum 来检查数据完整性 每个 chunk 分为 64KB 的 block，每个 block 对应一个 32 位的 checksum 读操作时检查 checksum 是否正确, 错误时返回 client 错误信息, 通知 master，从其它副本恢复数据 checksum 效验不需要额外 IO，对性能影响小 checksum 对 chunk 追加写入操作做了优化，只增量更新最后一个不完整 block 的 checksum chunkserver 空闲时扫描和校验不活动的 chunk  参考链接  MIT 6.824（二）GFS的一致性模型  ","wordCount":"669","inLanguage":"en","datePublished":"2021-08-28T00:00:00Z","dateModified":"2021-08-28T00:00:00Z","author":{"@type":"Person","name":"yuler"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhangyh.me/posts/paper/aug/gfs/"},"publisher":{"@type":"Organization","name":"yuler's blog","logo":{"@type":"ImageObject","url":"https://zhangyh.me/icon/jiaran16.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://zhangyh.me/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://zhangyh.me/archives/ title=archives>
<span>archives</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/categories/ title=categories>
<span>categories</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/search/ title="search (Alt + /)" accesskey=/>
<span>search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://zhangyh.me/>Home</a>&nbsp;»&nbsp;<a href=https://zhangyh.me/posts/>Posts</a></div>
<h1 class=post-title>
The Google File System
</h1>
<div class=post-meta>8-28&nbsp;·&nbsp;yuler&nbsp;|&nbsp;<a href=https://github.com/yu1er/blog-src/tree/main/content/posts/paper/aug/GFS.md rel="noopener noreferrer" target=_blank>Edit</a>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#1-introduction aria-label="1 Introduction">1 Introduction</a></li>
<li>
<a href=#2-design-overview aria-label="2 Design Overview">2 Design Overview</a><ul>
<li>
<a href=#21-assumptions aria-label="2.1 Assumptions">2.1 Assumptions</a></li>
<li>
<a href=#22-interface aria-label="2.2 Interface">2.2 Interface</a></li>
<li>
<a href=#23-architecture aria-label="2.3 Architecture">2.3 Architecture</a></li>
<li>
<a href=#24-single-master aria-label="2.4 Single Master">2.4 Single Master</a></li>
<li>
<a href=#25-chunk-size aria-label="2.5 Chunk Size">2.5 Chunk Size</a></li>
<li>
<a href=#26-metadata aria-label="2.6 Metadata">2.6 Metadata</a><ul>
<li>
<a href=#261-in-memory-data-structures aria-label="2.6.1 In-Memory Data Structures">2.6.1 In-Memory Data Structures</a></li>
<li>
<a href=#262-chunk-locations aria-label="2.6.2 Chunk Locations">2.6.2 Chunk Locations</a></li>
<li>
<a href=#263-operation-log aria-label="2.6.3 Operation Log">2.6.3 Operation Log</a></li></ul>
</li>
<li>
<a href=#27-consistency-model aria-label="2.7 Consistency Model">2.7 Consistency Model</a><ul>
<li>
<a href=#271-guarantee-by-gfs aria-label="2.7.1 Guarantee by GFS">2.7.1 Guarantee by GFS</a></li>
<li>
<a href=#272-implications-of-applications aria-label="2.7.2 Implications of Applications">2.7.2 Implications of Applications</a></li></ul>
</li></ul>
</li>
<li>
<a href=#3-system-interactions aria-label="3 System Interactions">3 System Interactions</a><ul>
<li>
<a href=#31-leases-and-mutation-order aria-label="3.1 Leases and Mutation Order">3.1 Leases and Mutation Order</a></li>
<li>
<a href=#32-data-flow aria-label="3.2 Data Flow">3.2 Data Flow</a></li>
<li>
<a href=#33-atomic-record-appends aria-label="3.3 Atomic Record Appends">3.3 Atomic Record Appends</a></li>
<li>
<a href=#34-snapshot aria-label="3.4 Snapshot">3.4 Snapshot</a></li></ul>
</li>
<li>
<a href=#4-master-operation aria-label="4. Master Operation">4. Master Operation</a><ul>
<li>
<a href=#41--namespace-management-and-locking aria-label="4.1  Namespace Management and Locking">4.1 Namespace Management and Locking</a></li>
<li>
<a href=#42-replica-placement aria-label="4.2 Replica Placement">4.2 Replica Placement</a></li>
<li>
<a href=#43-creation-re-replication-rebalancing aria-label="4.3 Creation, Re-replication, Rebalancing">4.3 Creation, Re-replication, Rebalancing</a></li>
<li>
<a href=#44-garbage-collection aria-label="4.4 Garbage Collection">4.4 Garbage Collection</a><ul>
<li>
<a href=#441-mechanism aria-label="4.4.1 Mechanism">4.4.1 Mechanism</a></li>
<li>
<a href=#442-discussion aria-label="4.4.2 Discussion">4.4.2 Discussion</a></li></ul>
</li>
<li>
<a href=#45-stale-replica-detection aria-label="4.5 Stale Replica Detection">4.5 Stale Replica Detection</a></li></ul>
</li>
<li>
<a href=#5-fault-tolerance-and-diagnosis aria-label="5. Fault Tolerance and diagnosis">5. Fault Tolerance and diagnosis</a><ul>
<li>
<a href=#51-high-availablity aria-label="5.1 High Availablity">5.1 High Availablity</a><ul>
<li>
<a href=#511-fast-recovery aria-label="5.1.1 Fast Recovery">5.1.1 Fast Recovery</a></li>
<li>
<a href=#512-chunk-replication aria-label="5.1.2 Chunk Replication">5.1.2 Chunk Replication</a></li>
<li>
<a href=#513-master-replication aria-label="5.1.3 Master Replication">5.1.3 Master Replication</a></li></ul>
</li>
<li>
<a href=#52-data-integrity aria-label="5.2 Data Integrity">5.2 Data Integrity</a></li></ul>
</li>
<li>
<a href=#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5 aria-label=参考链接>参考链接</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h2>
<p>本文是《The Google File System》的学习笔记</p>
<h2 id=2-design-overview>2 Design Overview<a hidden class=anchor aria-hidden=true href=#2-design-overview>#</a></h2>
<h3 id=21-assumptions>2.1 Assumptions<a hidden class=anchor aria-hidden=true href=#21-assumptions>#</a></h3>
<p>GFS基于以下基本假设作为设计原则。</p>
<ul>
<li>廉价商用机器出故障是<strong>常态</strong></li>
<li>主要存储大文件，通常大于<strong>100MB</strong>，没有针对小文件的优化</li>
<li>读操作：大规模的流式读、小规模的随机读</li>
<li>写操作：大规模的<strong>append</strong>写操作、小规模的随机写效率低</li>
<li>支持多个客户端并行 append 到同一个文件</li>
<li><strong>持续的高带宽</strong>比低时延重要</li>
</ul>
<h3 id=22-interface>2.2 Interface<a hidden class=anchor aria-hidden=true href=#22-interface>#</a></h3>
<ul>
<li>未实现POSIX API</li>
<li>支持常见文件操作，包括create, delete, open, close, read, write.</li>
<li>包括 snapshot 和 record append</li>
</ul>
<h3 id=23-architecture>2.3 Architecture<a hidden class=anchor aria-hidden=true href=#23-architecture>#</a></h3>
<p><img loading=lazy src=/img/gfs-arch.png alt=GFS架构>
</p>
<ul>
<li>集群通常包括一个 master、多个 chunkserver 和多个访问的 client</li>
<li>文件使用固定大小(64 MB)的 chunk 存储，使用<strong>globally unique、immutable</strong>的 64bit <em>chunk handle</em>表示</li>
<li>master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect</li>
<li>client 以 lib 的形式嵌入到应用中，以供 application 调用</li>
<li>client 和 chunkserver 都不 cache file data，但 client 会 cache metadata</li>
</ul>
<h3 id=24-single-master>2.4 Single Master<a hidden class=anchor aria-hidden=true href=#24-single-master>#</a></h3>
<ul>
<li>单 mater 简化了设计，但有成为 bottleneck 的风险</li>
<li>client 会缓存 metadata，从而减少和 master 的通信</li>
</ul>
<h3 id=25-chunk-size>2.5 Chunk Size<a hidden class=anchor aria-hidden=true href=#25-chunk-size>#</a></h3>
<p><strong>chunk size is one of the key design parameters</strong>
大 chunk size 的advantages</p>
<ol>
<li>减少 client 和 master 之间对于 metadata 通信</li>
<li>client 对 chunkserver 的某一 chunk 进行操作时，通过 tcp 持久连接减少网络负载</li>
<li>减少了 metadata 在 master 存储的大小
大 chunk size 的 disadvantages</li>
<li>对于小文件产生的 chunk 少，多个 client 的时可能会形成 hot spot。 但目前来看不是主要问题</li>
</ol>
<h3 id=26-metadata>2.6 Metadata<a hidden class=anchor aria-hidden=true href=#26-metadata>#</a></h3>
<p>有三种主要类型的 metadata</p>
<ol>
<li>file and namespace</li>
<li>file -> chunk handle</li>
<li>chunk handle -> locations of each replica</li>
</ol>
<ul>
<li>metadata 全部存储在 master 的内存中</li>
<li>1和2会以 operation log 的形式持久化到磁盘和远端机器中</li>
<li>3不持久化到磁盘中，master 启动的时候会和所有的 chunkserver 通信来获取3，之后在运行中通过 heartbeat 来维护</li>
</ul>
<h4 id=261-in-memory-data-structures>2.6.1 In-Memory Data Structures<a hidden class=anchor aria-hidden=true href=#261-in-memory-data-structures>#</a></h4>
<p>全内存有利于进行快速的 scan，通过 scan 我们能够完成以下操作。</p>
<ol>
<li>chunk garbage collection (参考<a href=#442-discussion>section 4.4.2</a>)</li>
<li>chunkerserver 故障时的 chunk 再复制</li>
<li>在 chunkserver 间的 chunk 迁移，用来平衡负载和磁盘使用率</li>
</ol>
<p>全内存导致chunk的数量受限于 master 内存的大小，但作者有以下的考量</p>
<ol>
<li>64 bit 即可管理一个 64MB 的 chunk</li>
<li>大部分 chunk 都是满的 ( full )</li>
<li>单纯增加内存的开销比较低 (与增加整个系统的复杂度相比)</li>
</ol>
<h4 id=262-chunk-locations>2.6.2 Chunk Locations<a hidden class=anchor aria-hidden=true href=#262-chunk-locations>#</a></h4>
<p>chunk location 信息由 chunkserver 来维护，master 通过 hearbeat 通信来获取，并存放到 metadata 中<br>
但 chunk locations 并不永久存放在 master 中，参考 <a href=#26-metadata>section 2.6</a><br>
简化了 chunkserver 加入、退出、更名、故障、重启的时候，master 与 chunkserver 同步的问题</p>
<h4 id=263-operation-log>2.6.3 Operation Log<a hidden class=anchor aria-hidden=true href=#263-operation-log>#</a></h4>
<ul>
<li>日志记录了重要的 metadata 的变动, 被持久化到本地和多台远程机器上</li>
<li>只有当日志持久化成功时，才会认为整个操作完成，返回成功给客户端</li>
<li>为了加快 master 的启动，会设置 checkpoint，checkpoint 以 b树 存储，可以直接映射到 master 内存中</li>
<li>master 通过最新的 checkpoint 和 checkpoint 之后的日志来进行数据恢复</li>
</ul>
<h3 id=27-consistency-model>2.7 Consistency Model<a hidden class=anchor aria-hidden=true href=#27-consistency-model>#</a></h3>
<p>GFS采用较宽松的一致性模型</p>
<h4 id=271-guarantee-by-gfs>2.7.1 Guarantee by GFS<a hidden class=anchor aria-hidden=true href=#271-guarantee-by-gfs>#</a></h4>
<ul>
<li>文件命名空间的改动是原子的</li>
<li>namespace lock 保证了原子性和正确性 <a href=#41->section 4.1</a></li>
</ul>
<p>一致性模型的定义<br>
<img loading=lazy src=/img/gfs-consistency.png alt=一致性模型>
定义：</p>
<ul>
<li>consistent：如果所有客户端无论从哪个副本读，读到的数据都是一样的，那么就认为这个数据是一致的。</li>
<li>defined：修改文件之后，相关数据是一致的，并且客户端能够读取到它刚才修改的内容，那么相关数据是已定义的。已定义暗含了一致。</li>
</ul>
<p>GFS 提供一个弱一致性模型。对于并发随机写入操作来说，数据是一致的，但是未定义的。对于追加写入操作来说，客户端总是可以读取到一致并且已定义的数据，但是实际储存在各个 chunk 服务器上的数据可能有部分是不一致的。</p>
<h4 id=272-implications-of-applications>2.7.2 Implications of Applications<a hidden class=anchor aria-hidden=true href=#272-implications-of-applications>#</a></h4>
<p>GFS 通过以下几点保证数据一致性：</p>
<ul>
<li>对 chunk 的所有副本修改操作顺序一致。</li>
<li>使用 chunk 版本号来检测副本因为服务器宕机而失效。</li>
<li>与服务器定期握手来找到失效的 chunk 服务器。</li>
<li>使用 checksum 来校验数据是否损坏。</li>
</ul>
<h2 id=3-system-interactions>3 System Interactions<a hidden class=anchor aria-hidden=true href=#3-system-interactions>#</a></h2>
<p>原则：最小化所有操作与master的交互</p>
<h3 id=31-leases-and-mutation-order>3.1 Leases and Mutation Order<a hidden class=anchor aria-hidden=true href=#31-leases-and-mutation-order>#</a></h3>
<ul>
<li>多个 replica 中只有一个获得 lease，因此只有一个 primary，其余为 secondary</li>
<li>由 primary 决定 chunk 变更的顺序，secondary 按照 primary 的要求，按照指定顺序进行变更</li>
<li>lease 默认有 60s 的租期，同一时刻只有一台 primary 持有 lease<br>
<img loading=lazy src=/img/gfs-write.png alt=gfs写入流程>
</li>
</ul>
<p>写入流程</p>
<ol>
<li>client 向 master 询问持有 chunk 租约的 chunkserver 以及 chunk 其它副本的位置。 如果没有，master 选择chunk 的一个副本建立租约</li>
<li>master 将主 chunkserver 的标识和其它副本的位置返回给 client，client 进行缓存</li>
<li>client 将<strong>数据</strong>推送到<strong>所有副本</strong>。chunkserver 将数据保存在内部的 LRU 缓存，此时并没有写入磁盘</li>
<li>一旦所有副本回复已经收到数据，client 向 primary 发送写请求。primary 向接收到的所有操作分配连续的序列号，操作可能来自不同 client</li>
<li>primary 把写请求发送给所有 secondary，secondary 按照主副本分配的序列号以相同的顺序执行</li>
<li>secondary 向主副本回复完成操作</li>
<li>primary 回复 client。任何副本的 error 都会返回，client 重新执行 3-7 的操作</li>
</ol>
<h3 id=32-data-flow>3.2 Data Flow<a hidden class=anchor aria-hidden=true href=#32-data-flow>#</a></h3>
<ul>
<li>data flow 与 control flow 分离，提高网络效率</li>
<li>使用 chain 式的发送，而不是 tree 式，从而充分利用每台机器的带宽，避免瓶颈</li>
<li>尽量选择没有接受数据且离自己最近的机器发送数据，通过 ip address 来估计机器之间的距离</li>
<li>采用 tcp 来发送数据</li>
</ul>
<h3 id=33-atomic-record-appends>3.3 Atomic Record Appends<a hidden class=anchor aria-hidden=true href=#33-atomic-record-appends>#</a></h3>
<ul>
<li>GFS 保证追加记录的原子性。当存在多个并发的追加记录时，GFS 对于每个追加记录至少有一次是写入成功的。GFS 指定写入的 offset 并且在之后返回，所有副本中的 offset 是一致的。</li>
<li>追加记录在之前描述的控制流程之上加了一些额外的步骤。对于一次指定的追加记录来说，主 chunk 服务器会检查给定 chunk 的大小。如果追加记录使得这个 chunk 的大小超过限制，服务器会填充这个 chunk 到最大大小然后指示客户端发起请求写入下一个 chunk。</li>
<li>如果追加记录在任意一个 chunk 服务器失败了，客户端需要进行重新操作。重新操作的结果是，同一个 chunk 的副本可能拥有不一致的记录。GFS 不保证所有副本在字节序上是完全一致的，但至少保证有一次成功的写入。</li>
<li>因此，追加记录将使数据是一致的，但可能包含部分不一致的局部数据，如填充和错误。</li>
</ul>
<h3 id=34-snapshot>3.4 Snapshot<a hidden class=anchor aria-hidden=true href=#34-snapshot>#</a></h3>
<p>copy-on-write</p>
<ul>
<li>snapshot 前，首先收回相关 chunk 的 lease，这保证了需要修改 chunk 时，首先要和 master 交互，从而在修改之前进行复制</li>
<li>master 维持一个对所有 chunk 的引用计数。当客户端发起一次快照请求时，并不实际进行复制，而是将相应引用计数加一。当对相应服务器发起写入请求时，master 注意到它的引用计数大于一，然后才复制一个新的 chunk，并指示客户端写入这个新 chunk。引用计数减一</li>
</ul>
<h2 id=4-master-operation>4. Master Operation<a hidden class=anchor aria-hidden=true href=#4-master-operation>#</a></h2>
<h3 id=41--namespace-management-and-locking>4.1 Namespace Management and Locking<a hidden class=anchor aria-hidden=true href=#41--namespace-management-and-locking>#</a></h3>
<ul>
<li>GFS 的名称空间是一个全局路径和元数据映射关系的查找表。这个表使用前缀压缩储存在内存中。在名称空间的属性结构上，每个节点都是都有一个关联的读写锁。</li>
<li>每一个master操作在执行前，都需要获取锁。假设我们要操作 /d1/d2/&mldr;/dn/leaf,需要先获取 /d1, /d1/d2, &mldr;,/d1/d2/&mldr;/dn的读锁，在获取 /d1/d2/&mldr;/dn/leaf 的写锁或读锁</li>
</ul>
<h3 id=42-replica-placement>4.2 Replica Placement<a hidden class=anchor aria-hidden=true href=#42-replica-placement>#</a></h3>
<p>副本放置的两个主要原则：</p>
<ol>
<li>最大化数据的可靠性和可用性</li>
<li>最大化网络带宽的利用率</li>
</ol>
<h3 id=43-creation-re-replication-rebalancing>4.3 Creation, Re-replication, Rebalancing<a hidden class=anchor aria-hidden=true href=#43-creation-re-replication-rebalancing>#</a></h3>
<p>chunk被创建通常处于三种目的<strong>creation</strong>, <strong>re-replication</strong>, <strong>rebalancing</strong></p>
<p>chunk creation 存放位置, 按优先级考虑以下三种因素：</p>
<ol>
<li>chunkserver 的磁盘使用率（与平均水平相比）</li>
<li>chunkserver 上最近创建 chunk 的数量</li>
<li>chunkserver 的地理位置分布（是否在一个 rack 上）</li>
</ol>
<p>当副本数量低于用户指定的水平是，会进行副本的 re-replication<br>
优先考虑以下的 chunk 进行 re-replication</p>
<ol>
<li>优先复制总数量更少的 chunk</li>
<li>优先复制活跃的 chunk</li>
<li>优先复制阻塞 client 流程的 chunk</li>
</ol>
<p>会定期进行 chunk 的 rebalancing，是每台 chunkserver 都拥有大致相同的磁盘使用水平，以充分利用磁盘</p>
<h3 id=44-garbage-collection>4.4 Garbage Collection<a hidden class=anchor aria-hidden=true href=#44-garbage-collection>#</a></h3>
<p>惰性的垃圾回收，使得整个系统更为简单</p>
<h4 id=441-mechanism>4.4.1 Mechanism<a hidden class=anchor aria-hidden=true href=#441-mechanism>#</a></h4>
<p>文件删除流程</p>
<ol>
<li>记录删除操作的 log</li>
<li>将文件 rename 成为一个包含时间戳信息的隐藏的名字</li>
<li>master 对 metadata 进行 scan 时，删除3天前被删除文件的元数据 （也就是三天内可以对文件进行恢复，这个时间是可以配置的）</li>
<li>master 通过与 chunkserver 通信，彻底删除磁盘上的文件</li>
</ol>
<h4 id=442-discussion>4.4.2 Discussion<a hidden class=anchor aria-hidden=true href=#442-discussion>#</a></h4>
<p>GFS的垃圾回收较为容易，通过 file->arrays of chunk handles 的映射找出所有未被使用的 chunk， 通过 delay delete 的方式删除（即<a href=#441-mechanism>4.4.1</a>所说）</p>
<p>advantages:</p>
<ol>
<li>在大规模系统中简单可靠，</li>
<li>多个删除操作在 master 较为空闲时进行后台批操作，分散开销</li>
<li>delay delete 为错误删除提供了安全保障</li>
</ol>
<p>disadvantages:</p>
<ol>
<li>delay delete 在磁盘空间紧张时阻碍了用户的使用</li>
<li>对于重复进行 create 和 delete 的应用不能马上释放资源（GFS可以配置指定目录下的文件删除策略——如即时删除、无副本）</li>
</ol>
<h3 id=45-stale-replica-detection>4.5 Stale Replica Detection<a hidden class=anchor aria-hidden=true href=#45-stale-replica-detection>#</a></h3>
<ul>
<li>每一个chunk，在master中都维护了一个 version number，同时 chunkserver 中也记录了当前的 version</li>
<li>master 与 chunk 签订租约时增加版本号，然后通知副本( Replica )</li>
<li>master 在垃圾回收的过程中移除过期失效的版本，在平时的读写操作中不会考虑过期版本</li>
</ul>
<h2 id=5-fault-tolerance-and-diagnosis>5. Fault Tolerance and diagnosis<a hidden class=anchor aria-hidden=true href=#5-fault-tolerance-and-diagnosis>#</a></h2>
<p>最大挑战之一：频繁的组件故障</p>
<h3 id=51-high-availablity>5.1 High Availablity<a hidden class=anchor aria-hidden=true href=#51-high-availablity>#</a></h3>
<p>保证高可用的两个策略：fast recovery 和 replication.</p>
<h4 id=511-fast-recovery>5.1.1 Fast Recovery<a hidden class=anchor aria-hidden=true href=#511-fast-recovery>#</a></h4>
<ul>
<li>数秒内恢复</li>
<li>不区分异常关闭和正常关闭</li>
</ul>
<h4 id=512-chunk-replication>5.1.2 Chunk Replication<a hidden class=anchor aria-hidden=true href=#512-chunk-replication>#</a></h4>
<ul>
<li>可以为不同的 namespace 设置不同的复制等级，默认为3</li>
<li>奇偶校验和纠删码</li>
<li>其他的冗余方案</li>
</ul>
<h4 id=513-master-replication>5.1.3 Master Replication<a hidden class=anchor aria-hidden=true href=#513-master-replication>#</a></h4>
<ul>
<li>master 所有操作日志和 checkpoint 备份在多个机器上</li>
<li>每个操作只有在写入磁盘和多个 master 备份后，才算生效</li>
<li>master 启动备用机器，通过 dns 别名的方式更改 client 指向为对 master 备用机的访问</li>
<li>影子 master 提供只读服务，通常由几秒分之一的滞后，通常体现在 metadata 上</li>
</ul>
<h3 id=52-data-integrity>5.2 Data Integrity<a hidden class=anchor aria-hidden=true href=#52-data-integrity>#</a></h3>
<ul>
<li>chunkserver 使用 checksum 来检查数据完整性</li>
<li>每个 chunk 分为 64KB 的 block，每个 block 对应一个 32 位的 checksum</li>
<li>读操作时检查 checksum 是否正确, 错误时返回 client 错误信息, 通知 master，从其它副本恢复数据</li>
<li>checksum 效验不需要额外 IO，对性能影响小</li>
<li>checksum 对 chunk 追加写入操作做了优化，只增量更新最后一个不完整 block 的 checksum</li>
<li>chunkserver 空闲时扫描和校验不活动的 chunk</li>
</ul>
<h2 id=参考链接>参考链接<a hidden class=anchor aria-hidden=true href=#参考链接>#</a></h2>
<ol>
<li><a href=https://blog.wonter.net/posts/aa8a2f6c/>MIT 6.824（二）GFS的一致性模型</a></li>
</ol>
</div>
<footer class=post-footer>
<ul class=post-tags>
</ul>
<nav class=paginav>
<a class=prev href=https://zhangyh.me/posts/leetcode/biweekly-contest-61/>
<span class=title>« Prev Page</span>
<br>
<span>biweekly-contest-61</span>
</a>
<a class=next href=https://zhangyh.me/posts/misc/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/>
<span class=title>Next Page »</span>
<br>
<span>聊聊内存对齐</span>
</a>
</nav>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://zhangyh.me/>yuler's blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>