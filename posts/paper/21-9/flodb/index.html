<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>FloDB: Unlocking Memory in Persistent Key-Value Stores | yuler's blog</title>
<meta name=keywords content="LSM-Tree">
<meta name=description content="Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化
  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。
  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。
Get 搜索顺序：Membuffer -> Memtable -> SStable
Update Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）
替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。
Scan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。
FloDB Implementation 明确了四各方面:
 Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer -> Hash Table Memtable -> Skiplist
Interaction Between Levels 包括persisting和draining.
Skiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。
批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。
并行性。插入和insert、read并行。
key的临近度决定了path-reuse的效果。">
<meta name=author content="yuler">
<link rel=canonical href=https://zhangyh.me/posts/paper/21-9/flodb/>
<meta name=google-site-verification content="G-JP3WQ36T5K">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabcsadf">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<link rel=icon href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=16x16 href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=32x32 href=https://zhangyh.me/icon/jiaran32.ico>
<link rel=apple-touch-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.2">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JP3WQ36T5K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JP3WQ36T5K',{anonymize_ip:!1})}</script>
<meta property="og:title" content="FloDB: Unlocking Memory in Persistent Key-Value Stores">
<meta property="og:description" content="Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化
  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。
  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。
Get 搜索顺序：Membuffer -> Memtable -> SStable
Update Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）
替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。
Scan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。
FloDB Implementation 明确了四各方面:
 Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer -> Hash Table Memtable -> Skiplist
Interaction Between Levels 包括persisting和draining.
Skiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。
批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。
并行性。插入和insert、read并行。
key的临近度决定了path-reuse的效果。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://zhangyh.me/posts/paper/21-9/flodb/"><meta property="og:image" content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-09-26T00:00:00+00:00">
<meta property="article:modified_time" content="2021-09-26T00:00:00+00:00"><meta property="og:site_name" content="yuler's blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="FloDB: Unlocking Memory in Persistent Key-Value Stores">
<meta name=twitter:description content="Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化
  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。
  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。
Get 搜索顺序：Membuffer -> Memtable -> SStable
Update Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）
替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。
Scan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。
FloDB Implementation 明确了四各方面:
 Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer -> Hash Table Memtable -> Skiplist
Interaction Between Levels 包括persisting和draining.
Skiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。
批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。
并行性。插入和insert、read并行。
key的临近度决定了path-reuse的效果。">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zhangyh.me/posts/"},{"@type":"ListItem","position":2,"name":"FloDB: Unlocking Memory in Persistent Key-Value Stores","item":"https://zhangyh.me/posts/paper/21-9/flodb/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FloDB: Unlocking Memory in Persistent Key-Value Stores","name":"FloDB: Unlocking Memory in Persistent Key-Value Stores","description":"Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化\n  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。\n  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。\nGet 搜索顺序：Membuffer -\u0026gt; Memtable -\u0026gt; SStable\nUpdate Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）\n替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。\nScan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。\nFloDB Implementation 明确了四各方面:\n Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer -\u0026gt; Hash Table Memtable -\u0026gt; Skiplist\nInteraction Between Levels 包括persisting和draining.\nSkiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。\n批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。\n并行性。插入和insert、read并行。\nkey的临近度决定了path-reuse的效果。","keywords":["LSM-Tree"],"articleBody":"Shortcomings of current LSMs   线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化\n  内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。\n  FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。\nGet 搜索顺序：Membuffer - Memtable - SStable\nUpdate Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）\n替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。\nScan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。\nFloDB Implementation 明确了四各方面:\n Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现  Memory Component Implementation Membuffer - Hash Table Memtable - Skiplist\nInteraction Between Levels 包括persisting和draining.\nSkiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。\n批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。\n并行性。插入和insert、read并行。\nkey的临近度决定了path-reuse的效果。\n","wordCount":"62","inLanguage":"en","datePublished":"2021-09-26T00:00:00Z","dateModified":"2021-09-26T00:00:00Z","author":{"@type":"Person","name":"yuler"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhangyh.me/posts/paper/21-9/flodb/"},"publisher":{"@type":"Organization","name":"yuler's blog","logo":{"@type":"ImageObject","url":"https://zhangyh.me/icon/jiaran16.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://zhangyh.me/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://zhangyh.me/archives/ title=archives>
<span>archives</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/categories/ title=categories>
<span>categories</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/search/ title="search (Alt + /)" accesskey=/>
<span>search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://zhangyh.me/>Home</a>&nbsp;»&nbsp;<a href=https://zhangyh.me/posts/>Posts</a></div>
<h1 class=post-title>
FloDB: Unlocking Memory in Persistent Key-Value Stores
</h1>
<div class=post-meta>9-26&nbsp;·&nbsp;yuler&nbsp;|&nbsp;<a href=https://github.com/yu1er/blog-src/tree/main/content/posts/paper/21-9/FloDB.md rel="noopener noreferrer" target=_blank>Edit</a>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#shortcomings-of-current-lsms aria-label="Shortcomings of current LSMs">Shortcomings of current LSMs</a></li>
<li>
<a href=#flodb-design aria-label="FloDB Design">FloDB Design</a><ul>
<li>
<a href=#get aria-label=Get>Get</a></li>
<li>
<a href=#update aria-label=Update>Update</a></li>
<li>
<a href=#scan aria-label=Scan>Scan</a></li></ul>
</li>
<li>
<a href=#flodb-implementation aria-label="FloDB Implementation">FloDB Implementation</a><ul>
<li>
<a href=#memory-component-implementation aria-label="Memory Component Implementation">Memory Component Implementation</a></li>
<li>
<a href=#interaction-between-levels aria-label="Interaction Between Levels">Interaction Between Levels</a></li>
<li>
<a href=#skiplist-multi-inserts aria-label="Skiplist Multi-inserts">Skiplist Multi-inserts</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=shortcomings-of-current-lsms>Shortcomings of current LSMs<a hidden class=anchor aria-hidden=true href=#shortcomings-of-current-lsms>#</a></h2>
<ol>
<li>
<p>线程的可扩展性
LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。
HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化</p>
</li>
<li>
<p>内存的可扩展性
内存中组件可以是<strong>有序跳表</strong>，也可以是<strong>无序哈希表</strong>，但二者都不适合应用到更大的内存中。
对于<strong>跳表</strong>而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。
对于<strong>哈希表</strong>而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。</p>
</li>
</ol>
<h2 id=flodb-design>FloDB Design<a hidden class=anchor aria-hidden=true href=#flodb-design>#</a></h2>
<p>设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。</p>
<h3 id=get>Get<a hidden class=anchor aria-hidden=true href=#get>#</a></h3>
<p>搜索顺序：Membuffer -> Memtable -> SStable</p>
<h3 id=update>Update<a hidden class=anchor aria-hidden=true href=#update>#</a></h3>
<p>Put和Delete操作和Update是相似的。
首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）</p>
<p>替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。</p>
<h3 id=scan>Scan<a hidden class=anchor aria-hidden=true href=#scan>#</a></h3>
<p>扫描Memtable和disk，同时允许Membuffer进行并行的update。
挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。
挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。</p>
<h2 id=flodb-implementation>FloDB Implementation<a hidden class=anchor aria-hidden=true href=#flodb-implementation>#</a></h2>
<p>明确了四各方面:</p>
<ol>
<li>Membuffer和Memtable数据结构的选择</li>
<li>数据从Membuffer到Memtable移动的机制</li>
<li>新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流</li>
<li>面向用户的操作的实现</li>
</ol>
<h3 id=memory-component-implementation>Memory Component Implementation<a hidden class=anchor aria-hidden=true href=#memory-component-implementation>#</a></h3>
<p>Membuffer -> Hash Table
Memtable -> Skiplist</p>
<h3 id=interaction-between-levels>Interaction Between Levels<a hidden class=anchor aria-hidden=true href=#interaction-between-levels>#</a></h3>
<p>包括persisting和draining.</p>
<h3 id=skiplist-multi-inserts>Skiplist Multi-inserts<a hidden class=anchor aria-hidden=true href=#skiplist-multi-inserts>#</a></h3>
<p>实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。</p>
<p>批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。</p>
<p>并行性。插入和insert、read并行。</p>
<p>key的临近度决定了path-reuse的效果。</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://zhangyh.me/tags/lsm-tree/>LSM-Tree</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://zhangyh.me/posts/cpp/priority-queue-custom-comparator/>
<span class=title>« Prev Page</span>
<br>
<span>priority_queue自定义排序</span>
</a>
<a class=next href=https://zhangyh.me/posts/paper/21-9/sfm/>
<span class=title>Next Page »</span>
<br>
<span>SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores</span>
</a>
</nav>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://zhangyh.me/>yuler's blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>