<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on yuler&#39;s blog</title>
    <link>https://zhangyh.me/posts/</link>
    <description>Recent content in Posts on yuler&#39;s blog</description>
    <image>
      <url>https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 13 Aug 2021 14:21:59 +0000</lastBuildDate><atom:link href="https://zhangyh.me/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>聊聊内存对齐</title>
      <link>https://zhangyh.me/topics/misc/%E8%81%8A%E8%81%8A%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.html</link>
      <pubDate>Fri, 13 Aug 2021 14:21:59 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/misc/%E8%81%8A%E8%81%8A%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.html</guid>
      <description>为什么需要内存对齐？ 首先需要从物理硬件上了解计算机如何进行内存访问的。
 Channel &amp;gt; DIMM &amp;gt; Rank &amp;gt; Chip &amp;gt; Bank &amp;gt; Coloum/Row &amp;gt; Cell
 如上图 CPU包括两个Channel 每个Channel包括两个DIMM 每个DIMM由Rank组成 Rank由8个内存颗粒chip组成 每个Chip包括8个Bank
CPU读取内存时从8个chip中每个读取8bit字节，从而构成64bit column和row定位的一个单元格cell中有8个bit
因此一次性最少读取64bit，这恰好也是cacheline的大小。 （cacheline是cache的基本单位，每个cache由若干cacheline组成）
什么是内存对齐 从上面我们可以知道，一次性最少读取8B，这也是局部性原理的一种使用。
内存对齐就是代码编译后在内存中的布局，当一个内存地址刚好能够整除8，就称为其内存地址是8字节对齐的。
为什么需要内存对齐 对于go中定义的如下结构
type Type1 struct { a int8 b int64 c int32 } 其内存布局是这样的： 通过unsafe.Sizeof()可以打印出该结构占用了24字节的内存。
如果调整一下顺序：
type Type2 struct { a int8 c int32 b int64 } 便可以节约一个字节的内存
零大小字段 如果结构体或数组类型不包含大小大于零的字段或元素，那么它的大小就为0
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;unsafe&amp;#34; ) type M struct { m int64 x struct{} } type N struct { x struct{} n int64 } func main() { m, n := M{}, N{} fmt.</description>
    </item>
    
    <item>
      <title>LSM-based storage techniques: a survey</title>
      <link>https://zhangyh.me/topics/paper/suvery-of-lsm.html</link>
      <pubDate>Tue, 10 Aug 2021 02:47:09 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/suvery-of-lsm.html</guid>
      <description> paper原文：LSM-based storage techniques: a survey
 LSM-tree basics 对于索引结构通常有两种更新策略，即in-place update和out-of-place update。
in-place通常使用B+树作为底层数据结构，通过随机写来更新数据项，带来的是读数据的优化，同时每个数据项只存有一份，因此节约了空间。
out-place通常使用lsm tree作为底层数据结构，将随机写转化为顺序写，加快了写的速度，但同时降低了读数据的性能，另外，对同一个key，存有多个版本，带来了空间上的浪费。
LevelDB是谷歌开源的kv数据库，它基于内存-磁盘的存储层次，实现了LSM Tree最基本的功能。 内存中有memtable，使用skiplist来按顺序存储kv对 磁盘中有分层的sstable，使用sorted-string table以文件的形式来存储。 sstable包括了data block、index block、footer block
如今的lsm tree通常为了加快读速度，往往会具有bloom filter组件，该组件有两个操作：插入key和检查key是否存在。为磁盘中的每一level，维护一个bloom filter，从而提高读性能。需要注意，它是false positive的。 同时因为该结构比较小，往往把他缓存到内存当中。
对于failure recovery，使用WAL保证内存中memtable的recovery，使用manifest保证merge过程中sstable的recovery。
有两种常用的merge policy，如上图所示。 Leveling merge policy，要求每一层的SSTable之间不能overlap，merge的时候将第i层的SSTable和第i+1层overlape的所有SSTables进行merge，然后写入i+1层。也就是说通过leveling通过频繁的merge，使得每一层全部有序，牺牲部分写性能来换取读性能。
Tiering merge policy，要求每一层的SSTable之间可以overlap，在sstable达到一定的数量或者size要求后，对该层所有的SSTable进行merge sort，然后直接flush进下一层。该策略通过减少merge操作，牺牲部分读性能换取了写性能。
LSM-tree improvement 文章将对LSM-tree的优化分为以下几个方面：
 Write Amplification: out-of-place数据结构所具有的问题，降低了写性能和磁盘寿命。 Merge Operation: merge后会造成buffer cache miss，大数据量的merge会造成write stall Hardware: 针对large memory,multi-core, SSD/NVM, native storage的优化 Special Workloads: Auto-tuning: 不可能同时达到read,write,space同时最优，并且由于lsm tree拥有大量参数，手动调优也是困难的。 Secondary Indexing: lsm tree只支持简单的kv接口，二级索引也是一个方向。  </description>
    </item>
    
    <item>
      <title>Wiskey</title>
      <link>https://zhangyh.me/topics/paper/wiskey.html</link>
      <pubDate>Tue, 10 Aug 2021 02:47:09 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/wiskey.html</guid>
      <description>WiscKey Design Goals  Low写放大 Low读放大 SSD优化 Feature-rich API (scan、snapshot) Realistic k-v size (value的size通常比key的大很多)  K-V Separation  LSM tree的主要性能成本在于Compaction，但它维护了key的有序性，对于加速读也是十分有必要的。 Compaction只需要对key排序，因此考虑将k-v分开存储。只把value的地址和key放在一起。  LSM tree的size将大大变小，减少了写放大。 读操作虽然需要一次额外的寻址操作，但更小的LSM tree加快了检索同时更容易缓存到大内存中。    Challenge   Parallel Range Query 在levelDB中，范围查询通过iterator的顺序读实现，但是现在由于和key一同存储的只有value的地址，范围查询变为了随机io。 我们在读取所有的地址后，写入一个队列，通过多线程并发读取，来加快范围查询的性能。
  Garbage Collection 由于把所有的数据都存到了vLog里，最朴素的垃圾回收的方法当然是扫描一遍LSM-tree，但开销太大了，只适用于离线环境。 WiscKey在vlog中存储(ksize, vsize, key, value)，并维护head和tail指针。   数据在head处添加 垃圾回收线程从tail处扫描，每次扫描一定量的数据（几MB），然后在LSM-tree中查找，如果kv对有效的话，则再把它添加到head处。 因此，只有tail和head之间的数据是有效的。
Crash Consistency  Optimizations  Value-Log Write Buffer  对于写密集的数据集，频繁的大量的小size的value写入，会导致较大的系统开销。 因此考虑在内存中维护Buffer。 当读数据的时候，先到vlog write buffer中查找，如果没有，再查vlog。 crash-consistency的维护类似于levelDB，使用WAL。
Optimizing the LSM-tree Log  在写LSM-tree之前需要先写vlog，因为这里我们加了vlog write buffer，所以我们直接用vlog的日志作为WAL。</description>
    </item>
    
    <item>
      <title>Hashkv Enabling efficient updates in KV storage via hashing</title>
      <link>https://zhangyh.me/topics/paper/hashkv.html</link>
      <pubDate>Sat, 07 Aug 2021 02:47:09 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/hashkv.html</guid>
      <description>Hashkv: Enabling efficient updates in KV storage via hashing Introduction kv分离的Wisckey在vLog的GC上有着以下的问题：
 最新写入的数据被从tail迁移到head处，带来了写放大。 每次GC，对于每一项都需要去查询LSM-Tree，去验证是否还有效，造成大量随机读  Design Storage Management 固定大小的空间单元main segment和log segment。 默认分别为64MB和1MB.
segment group 包括一个main segment和多个log segment.
内存中全局的segment table存储每一个segment group接下来插入或更新的位置。
为了方便GC，segment group存储了key/value size，key，value
GC Collection GC以segment group为单元，当free log segment用尽的时候会触发。
 首先选择候选的segment group，并识别valid KV 然后将valid KV写新的main segment和log segment 再释放之前未使用的log segment 最后更新LSM-Tree中value的最新位置  两个问题：
 如何选择候选的segment group? 根据更新的kv数量，快速选择 如何快速验证KV时valid的 从尾部第一次读到的key，它对应的kv一定是最新的。  Hotness Awareness 在对segment group进行GC后，决定其中数据项为hot还是code。
hot数据仍然写回segment group. code数据写到另外一个区域，旨在segment group中保留其metadata()
如何判断冷热？ 在最后插入后，又更新过一次的即为热数据。
我们称呼该过程为tagging，注意，tagging只发生在GC时。</description>
    </item>
    
    <item>
      <title>A Light-weight Compaction Tree to Reduce I/O Amplification toward Efficient Key-Value Stores</title>
      <link>https://zhangyh.me/topics/paper/lwc-tree.html</link>
      <pubDate>Fri, 06 Aug 2021 02:47:09 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/lwc-tree.html</guid>
      <description>A Light-weight Compaction Tree to Reduce I/O Amplification toward Efficient Key-Value Stores Introduction 一般情况下的LSM Tree写放大能达到50X倍 缓解写放大的方法可能有以下几种：
 使用更大的内存buffer 利用设备的特性 逐步散列 KV 项 减少level层级 减少每层的空间放大因子  Design light-weight compaction 也是借鉴了wisckey中提到的，merge and sort过程只是面向key的，不需要value的参与 在对Level i 和 Level i+1进行compaction时，我们把第i层的候选的sstable成为victim，把与victim有overlap的i+1层的sstable成为overlaped. (注意该论文为sstable设计了另外一种存储格式，成为DTable) 因此整个lightweight compaction过程为:
 读取victim到内存中，victim包括了overlaped的metadata 然后根据metadata，将victim分为几个segment 再根据key range将segment追加到对应的DTable中  victim一层的写放大被消除了，随之带来的问题就是DTable的读，虽然每一层的DTable之间是无overlap且有序的，但是DTable内的key值并不是完全有序的。
降低了大概10X的写放大.
Metadata Aggregation overlaped的元数据如何获取。 直觉的方法是直接从overlaped读，但这带来的随机io违背了我们的设计初衷。 另一种是直接在内存中缓存元数据，但这部分开销并不算小，并不划算 为了解决这个问题，我们提出了元数据聚合。 在一次compaction完成后，第i+1层被更新的元数据存储到victim中，以此避免下一次compaction时对metadata的访问呢。
通过compaction过程中一次额外的写，减少AF倍的i+1层的随机读。
Data structure of DTable 每个DTable维护下层的overlaped Dtables metadata 每次compaction过程中append进来的数据作为一个segment,segment之间的key是overlap的,会导致查找性能的下降 MetadaBlock包括了bloom filter blocks,overlaped metada_index block, metada_index block,index block footer</description>
    </item>
    
    <item>
      <title>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</title>
      <link>https://zhangyh.me/topics/paper/optane.html</link>
      <pubDate>Thu, 05 Aug 2021 02:47:09 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/optane.html</guid>
      <description>Introduction 过去的很多研究都是基于仿真模拟的NVM，他们基于——NVM表现和DRAM类似，只不过性能相对较低——这样的假设完成了研究。
但实验发现，Optane DIMM的性能与DRAM相比，更依赖于
 access size access method(read,write) pattern degree of concurrency  本文通过经过实验证明了之前的很多仿真方法都是不可靠的。
Background Optane DIMM是第一款商用NVDIMM 与SSD/HDD相比，它low latency、higher read bandwidth、byte-address 与DRAM相比，它higher density、persistent
Optane Memory Optane DIMM使用和DRAM相同的插槽，和处理器的集成内存控制器（iMC)相连接。Intel Cascade Lake处理器是第一个支持Optane DIMM的处理器，有一个或两个processor die，每个die支持两个iMC，每个iMC支持三个channel，因此一个die可以支持6个Optane DIMM
为了持久化，iMC维护一个asynchronous DRAM refresh(ADR)区域，写入这个区域内的数据都能保证其被持久化。ADR区域不保证处理器cache的持久化。
iMC按照缓存行粒度(64byte)和Optane DIMM通信。
Optane DIMM中
3D-XPoint物理介质的访问粒度为256byte，因此会造成写放大，所以会在XPController中有controller负责将小于256B的操作变成256B的访问，同时内部有Buffer用来合并临近的访问。
为了磨损均匀、坏块的管理，AIT用于进行内部地址转换。
Operation Model Optane DIMM有两种模式Memory和App Direct
 Memory Model 该模式下直接将Optane DIMM作为普通的内存使用，将DRAM作为内存的Cache，DRAM是透明的，直接观察到的内存容量就是Optane DIMM的容量。 该模式可以解决一些内存数据库内存容量不足的问题。 App Direct Model 该模式将Optane DIMM作为持久化设备使用，直接通过CPU指令读写。文件系统和其他的管理层管理持久化内存的分配、回收和访问。  支持交错访问，最小的单位是4KB，保证一个page一定是从一个DIMM中读取出来的
ISA Support store: 绕过store buffer ntstore: 绕过CPU cache，直接写到内存。一般用于写完就不管的情况，可以防止污染cache。 clflush: 把cache line刷回内存，并且让cache line失效。只能串行执行。 clflushopt: 功能同clflush，但是不同缓存行可以并发执行。 clwb: 除了写回后不让cache line失效，其他同clflushopt。 sfence：写屏障，在sfence指令前的写操作当必须在sfence指令后的写操作前完成</description>
    </item>
    
    <item>
      <title>Redesigning LSMs for nonvolatile memory with NoveLSM</title>
      <link>https://zhangyh.me/topics/paper/novelsm.html</link>
      <pubDate>Tue, 03 Aug 2021 07:58:56 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/novelsm.html</guid>
      <description>Introduction 和目前的存储技术（例如flash、硬盘）相比， NVM有着以下的没有被LSM考虑的优点：
 对于持久存储的随机访问的高性能 in-place update的低成本 (具体是?) 低时延、高带宽为application-level的并行化提供了机会  作者认为探索redesign适用于NVM的LSM是有意义的，而不是design a new data structure。 基于以下考量：
 未来几年NVM和SSD共存，形成异质存储，而不是完全取代。redesign LSM可以在利用NVM的优点的前提下，同时不失去ssd和硬盘最优化的优势。 redesign lsm能为现有的应用提供向后兼容 保证批量写入NVM同样重要（NVM写延迟为DRAM的5-10倍）  Motivation 单纯的硬件NVM的读写是SSD的100倍左右，但在LevelDB在NVM和SSD上的差异只有4-7倍 因此可以说目前的LSM没有充分利用NVM的硬件优势，软件开销较大
Insert Latency insert latency来源有三个方面：
 WAL memtable insert compaction   对于compaction 内存中的mutable memtable在写满之后会刷新成immutable memtable，由后台进程将其压缩进磁盘，同时新开一个mutable memtable，来接管写入 问题是新的mutable memtable写满之后，如果immutable memtable还没有刷到磁盘内，就会造成系统的停顿 当进行大量的写入时，这会成为insert latency的主要来源。
 可能会说，让memtable大一点不就可以解决，但这会带来一系列的问题：
 增大memtable会带来双倍的内存占用，因为mutable memtable和immutable memtable需要同时增大 WAL的磁盘占用也会更大，因为他需要容纳更多的指令 LSM为了性能并不commit log。   Third, LSMs suchas LevelDB and RocksDB do not enforce commits (sync) when writing to a log; as a result, an application crash or power-failure could lead to data loss</description>
    </item>
    
    <item>
      <title>Zen: a high-throughput log-free OLTP engine for non-volatile main memory</title>
      <link>https://zhangyh.me/topics/paper/zen.html</link>
      <pubDate>Tue, 03 Aug 2021 07:48:31 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/paper/zen.html</guid>
      <description>Design Overview  Hybrid Table(HTable)  tuple heap in NVM Met-Cache in DRAM per-thread NVM-tuple managers.   Metadata in NVM Transaction and Indices in DRAM  NVM heap 所有的tuple持久化到NVM中的tuple heap，它由大小为2MB的pages组成 其中可能同时存在某个数据的多个版本 (tupleId, Tx-CTS)唯一确定一个tuple
 LP: 在commited transaction中最后一个被持久化的tuple，将该标记记为1 Tx-CTS: 每个事务在线程内都有唯一ID，单调递增。 Deleted: 被删除  Met-Cache NVM heap在DRAM中的缓存
 Clock bit: 用于Clock置换策略 Active bit: 事务正在使用该tuple Dirty bit: 被修改，当事务中途被absort，根据dirty位重新到NVM Heap获取tuple Copy bit: tuple被复制 CC-Meta：用于DRAM中的并发控制时，存储对应的信息。Zen中的并发控制完全实现在DRAM中，支持多种并发控制策略。  Indices in DRAM 在DRAM维护索引。crash后会重建。索引指向Met-Cache或者NVM heap中的tuple
Transaction-Private Data 线程私有 用于事务并发访问 存储相应数据</description>
    </item>
    
    <item>
      <title>Go | 内存分配</title>
      <link>https://zhangyh.me/topics/golang/go-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D.html</link>
      <pubDate>Sat, 17 Jul 2021 12:14:57 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D.html</guid>
      <description>设计原理 Go的内存分配参考了TCMalloc的核心思想。
 每一个线程都可以获得一个用于无锁分配小对象的缓存，这样可以让并行程序分配小对象（&amp;lt;=32KB）非常高效。 TCMalloc 管理的堆由一组页组成，一组连续的页面被表示为 span。当分配的对象大于 32KB，将使用页堆（Page Heap）进行内存分配。  相关struct mspan mspan是Go语言内存管理的基本单元。
这里只列出了关心的主要字段。
 mspan使用 next和prev指针构成双向链表 startAddr确定了mspan所在内存的地址，npages确定了内存地址范围 每个mspan都管理这npages数量的page（注意这里的page不是操作系统的page，它是操作系统page的整数倍） state为mspan的状态，不细说。  spanClass 这是mspan中一个很核心的字段，spanClass是跨度类，它决定了mspan中管理的存储对象的大小和个数。
Go语言中一共有68种跨度类，每个跨度类会存储特定大小的对象并且需要分配指定数量的page。
class_to_size表示每个跨度类的字节大小。
class_to_allocnpages表示每个跨度类需要分配page的数量。
ID为0的特殊跨度类，用于管理大对象。
跨度类的最后一个二进制位是nospan标记，即表示对象是否包含指针，用于加快垃圾回收。
每个mspan的内存都用于指定跨度类的内存分配。在分配内存时，首先需要选择最适合的跨度类，从而避免内存浪费。
mcache mcache是线程缓存，它和GMP模型中的P绑定，主要用于为用户小于32KB的微、小对象分配内存。
每个mcache有 $68 * 2$(_NumSizeClasses &amp;lt;&amp;lt; 1) 个 mspan。
上面我们说过跨度类共有68种，这里每个跨度类又分为有指针和无指针两类。
对于小对象，在mcache中，为对象寻找mspan的流程如下：
 计算对象所需内存大小size 根据size到size class映射，计算出所需的size class 根据size class和对象是否包含指针计算出span class 获取该span class指向的span。  但是如果对应的mspan没有剩余空间可被分配时，mcache就需要通过refill()函数向mcentral申请mspan,mcache拿到mspan后继续分配内存。
mcentral 每个中心缓存都会管理某个跨度类的内存管理单元，它会同时持有两个 runtime.spanSet，分别存储包含空闲对象和不包含空闲对象的内存管理单元。
当mcentral中的span不够用时，会找到mheap分配页数并获取新的mspan结构。
mheap mheap是内存分配的核心结构体，每个Go语言程序有一个对应的全局变量。
mheap包含两个重要部分，一个是mcentral数组，它管理所有的跨度类内存。另一个是管理堆内存区域的arenas。
Go 语言所有的内存空间都由如下所示的二维矩阵 heapArena 管理，这个二维矩阵管理的内存可以是不连续的，每一个 heapArena 都会管理 64MB 的操作系统的内存空间。
内存分配  大于 32K 的大对象直接从 mheap 分配。 小于 16B 的使用 mcache 的微型分配器分配 对象大小在 16B ~ 32K 之间的的，首先通过计算使用的大小规格，然后使用 mcache 中对应大小规格的块分配 如果对应的大小规格在 mcache 中没有可用的块，则向 mcentral 申请 如果 mcentral 中没有可用的块，则向 mheap 申请，并根据 BestFit 算法找到最合适的 mspan。如果申请到的 mspan 超出申请大小，将会根据需求进行切分，以返回用户所需的页数。剩余的页构成一个新的 mspan 放回 mheap 的空闲列表。 如果 mheap 中没有可用 span，则向操作系统申请一系列新的页（最小 1MB）。 Go 会在操作系统分配超大的页（称作 arena）。分配一大批页会减少和操作系统通信的成本  References  https://draveness.</description>
    </item>
    
    <item>
      <title>Go | context</title>
      <link>https://zhangyh.me/topics/golang/go-context.html</link>
      <pubDate>Fri, 16 Jul 2021 10:25:31 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go-context.html</guid>
      <description>什么是context context 主要用来在goroutine之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。
可以通过以下个函数创建实现对应的功能:
 context.WithCancel(): 创建带有取消函数的context，上层goroutine调用cancelFunc函数，向下层传递取消信号 context.WithTimeout(): 创建带有超时的context，同时创建计时器，超时的时候调用context的cancelFunc；当然也可以主动调用cacelFunc context.WithDeadline(): 和带超时的context类似，实际底层WithTimeout就调用了WithDeadline； return WithDeadline(parent, time.Now().Add(timeout)) context.WithValue(): 创建带值的context  使用场景 1. 链路传值  注意值的流向只能从上到下，上层context是获取不到下层传入的值的 传入的context不能为nil，可以传context.Background()或context.TODO()  2. 链路并发控制 context最主要的作用是解决cancelation问题，也就是通过上层goroutine来释放下层goroutine。
可以把主要的并发模型归为两类，wait和cancel
Wait主要通过sync.WaitGroup实现，Cancel则需要通过context.Context来实现
这里只是用了带有cancel信号的context，当然也可以使用带有超时和截止时间的context
使用原则  不要将 Context 塞到结构体里。直接将 Context 类型作为函数的第一参数，而且一般都命名为 ctx。 不要向函数传入一个 nil 的 context，如果你实在不知道传什么，标准库给你准备好了一个 context.TODO() 不要把本应该作为函数参数的类型塞到 context 中，context 存储的应该是一些共同的数据。例如：登陆的 session、cookie 等。 同一个 context 可能会被传递到多个 goroutine，别担心，context 是并发安全的。  References  https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-context https://www.cnblogs.com/qcrao-2018/p/11007503.html https://36kr.com/p/1721518997505 https://faiface.github.io/post/context-should-go-away-go2/ https://golang.org/pkg/context/#example_WithDeadline https://xie.infoq.cn/article/3e18dd6d335d1a6ab552a88e8  </description>
    </item>
    
    <item>
      <title>Go | make和new</title>
      <link>https://zhangyh.me/topics/golang/go-make%E5%92%8Cnew.html</link>
      <pubDate>Sun, 11 Jul 2021 07:17:49 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go-make%E5%92%8Cnew.html</guid>
      <description>区别    方法 作用对象 返回值     new 值类型和用户定义类型 指向对象的指针   make 内置引用类型slice, map, channel 引用类型本身    new new用于为值类型或用户自定义的类型分配内存，并初始化零值，返回零值指针。
make make是为了初始化slice，map，channel这三种引用类型的。
针对这三种类型，分别调用runtime.makeslice，runtime.makemap，runtime.makechan函数。
make 相对于 new 来说，做的事情更多，new 只是开辟了内存空间， make 为更加复杂的数据结构开辟内存空间并对一些字段进行初始化
总结  new用于为值类型或用户自定义的类型 make只能用于slice,map,channel  new一般不常用，通常直接使用结构体字面量</description>
    </item>
    
    <item>
      <title>理解io多路复用</title>
      <link>https://zhangyh.me/topics/misc/%E7%90%86%E8%A7%A3io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.html</link>
      <pubDate>Sat, 03 Jul 2021 03:02:56 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/misc/%E7%90%86%E8%A7%A3io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.html</guid>
      <description>之前对io多路复用有诸多疑惑，看了很多文章还是不甚了解——它是什么，它要解决什么问题。最近刚好需要分析golang网络轮询器，趁此机会，把io多路复用的相关内容都总结记录一下。
为什么需要io多路复用模型? 当我们开启一个socket的时候，需要对发起的连接进行响应。
阻塞io 阻塞io流程如图 如果使用阻塞io，我们可能会有下面类似的代码框架。
listenfd = socket() // 创建socket bind(listenfd, addr) // 将socketfd和服务器地址绑定 listend(listenfd) // 转换为监听套接字  while(1) { connfd = accept(listenfd) // 阻塞建立连接  new thread func(){ int n = read(connfd, buf) // 阻塞读数据  do_something() // 业务代码  close(connfd) //关闭连接套接字  } } 在accept()建立起连接后，我们会使用多线程来接手连接套接字connfd，阻塞的读取客户端发送来的内容。
 每个连接都要开启一个线程来阻塞读取数据，但大多数线程都处于阻塞状态，造成了严重的线程浪费  非阻塞io 非阻塞io流程如图
如果使用非阻塞io，我们可能会有下面类似的代码框架
while(1) { connfd = accept(listenfd); // 阻塞建立连接  append(fds, connfd) for fd in fds { fcntl(connfd, F_SETFL, O_NONBLOCK); int n = read(connfd, buffer); if n !</description>
    </item>
    
    <item>
      <title>Go调度 | 4. 调度策略</title>
      <link>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-4-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5.html</link>
      <pubDate>Thu, 01 Jul 2021 16:45:55 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-4-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5.html</guid>
      <description>前文分析了scheduler的初始化以及goroutine创建和执行，略过了调度策略的内容。
有关调度策略的部分主要位于调度函数schedule()中，本文将具体分析。
schedule() 从p的本地队列获取goroutine 从源码上看，p的本地队列包括两部分，一个是runnext所指向的goroutine，另一个就是由runq,runhead和runtail组成的无锁队列。
具体实现的函数为runqget()
从schedt的全局队列获取goroutine  需要注意，每个工作线程每进行61次调度就需要优先从全局运行队列中获取
 working-stealing策略  findrunnable主要负责偷取G，代码十分繁杂，这里只分析我们主要关心的部分，忽略gc和netpoll相关的内容
  正在偷取g的工作线程处于spinning状态。 偷取时通过一种伪随机的方式遍历allp，然后在内层循环中通过runqsteal实现steal逻辑  从p2中偷取一半的g放到p中
如果无论如何都没有获取到可以运行的goroutine，则调用stopm进入睡眠状态，等待被其他的工作线程唤醒。
 stopm的核心是调用mput把m结构体对象放入sched的midle空闲队列，然后通过notesleep(&amp;amp;m.park)函数让自己进入睡眠状态。
note是go runtime实现的一次性睡眠和唤醒机制，一个线程可以通过调用notesleep(_note)进入睡眠状态，而另外一个线程则可以通过notewakeup(_note)把其唤醒。note的底层实现机制跟操作系统相关，不同系统使用不同的机制。
回到stopm，当从notesleep函数返回后，需要再次绑定一个p，然后返回到findrunnable函数继续重新寻找可运行的goroutine，一旦找到可运行的goroutine就会返回到schedule函数，并把找到的goroutine调度起来运行，如何把goroutine调度起来运行的代码我们已经分析过了。现在继续看notesleep函数。
 </description>
    </item>
    
    <item>
      <title>Go调度 | 3. goroutine的创建和执行</title>
      <link>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-3-goroutine%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E6%89%A7%E8%A1%8C.html</link>
      <pubDate>Thu, 01 Jul 2021 04:36:30 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-3-goroutine%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E6%89%A7%E8%A1%8C.html</guid>
      <description>参考[上篇文章]()对文章对程序启动过程的分析，这里仍旧沿着上次分析的思路来分析main gorotine的创建和调度过程。
go程序的启动 main goroutine newproc() 该函数创建出main goroutine
newproc主要是对newproc1的封装。newproc首先获取要执行函数fn的参数地址argp，然后通过systemstack切换到g0栈去执行newproc1，然后把生成的g放入到绑定的p的本地队列中。
需要注意在这里由于我们当前已经是g0，因此不需要切换，但由于newproc函数是通用的，在用户的goroutine中也会被调用，因此这里使用systemstack进行栈切换是有必要的。
下面我们看一下newproc1做了什么。
到此时，我们已经完成main goroutine的创建，我们参考这篇文章，给出当前的状态图。
 这个图看起来比较复杂，因为表示指针的箭头实在是太多了，这里对其稍作一下解释。
 首先，main goroutine对应的newg结构体对象的sched成员已经完成了初始化，图中只显示了pc和sp成员，pc成员指向了runtime.main函数的第一条指令，sp成员指向了newg的栈顶内存单元，该内存单元保存了runtime.main函数执行完成之后的返回地址，也就是runtime.goexit函数的第二条指令，预期runtime.main函数执行完返回之后就会去执行runtime.exit函数的CALL runtime.goexit1(SB)这条指令； 其次，newg已经放入与当前主线程绑定的p结构体对象的本地运行队列，因为它是第一个真正意义上的goroutine，还没有其它goroutine，所以它被放在了本地运行队列的头部； 最后，newg的m成员为nil，因为它还没有被调度起来运行，也就没有跟任何m进行绑定。   ![图片来源：https://www.cnblogs.com/abozhang/p/10825342.html](https://cos.yuler.asia/img/post/main-goroutine.png
mstart() mstart没有什么多说的，主要封装了mstart1，我们下面分析mstart1。
schedule() execute() 此时还是在g0的栈上运行，通过gogo函数切换到gp的栈上运行
gogo()  gogo函数也是通过汇编语言编写的，这里之所以需要使用汇编，是因为goroutine的调度涉及不同执行流之间的切换，前面我们在讨论操作系统切换线程时已经看到过，执行流的切换从本质上来说就是CPU寄存器以及函数调用栈的切换，然而不管是go还是c这种高级语言都无法精确控制CPU寄存器的修改，因而高级语言在这里也就无能为力了，只能依靠汇编指令来达成目的。
 gogo的作用为：
 把gp.sched的成员恢复到CPU的寄存器完成状态以及栈的切换； 跳转到gp.sched.pc所指的指令地址（runtime.main）处执行。  runtime.main() runtime.main函数主要工作流程如下：
 启动一个sysmon系统监控线程，该线程负责整个程序的gc、抢占调度以及netpoll等功能的监控，在抢占调度一章我们再继续分析sysmon是如何协助完成goroutine的抢占调度的； 执行runtime包的初始化； 执行main包以及main包import的所有包的初始化； 执行main.main函数； 从main.main函数返回后调用exit系统调用退出进程；  非main goroutine main goroutine结束时会执行exit(0)直接结束进程，从而使得所有其余的goroutine被终止。
而非main goroutine结束时则会执行goexit()函数完成清理工作。
goexit() 调用runtime.goexit1()函数
goexit1函数通过调用mcall从当前运行的go goroutine切换到g0，然后在g0栈上调用和执行goexit0这个函数。
下面开始在g0栈执行goexit0函数，该函数完成最后的清理工作：
 把g的状态从_Grunning变更为_Gdead； 然后把g的一些字段清空成0值； 调用dropg函数解除g和m之间的关系，其实就是设置g-&amp;gt;m = nil, m-&amp;gt;currg = nil； 把g放入p的freeg队列缓存起来供下次创建g时快速获取而不用从内存分配。freeg就是g的一个对象池； 调用schedule函数再次进行调度；  总结  我们用上图来总结一下工作线程的执行流程：</description>
    </item>
    
    <item>
      <title>Go调度 | 2. scheduler初始化</title>
      <link>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-2-scheduler%E5%88%9D%E5%A7%8B%E5%8C%96.html</link>
      <pubDate>Wed, 30 Jun 2021 10:51:39 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-2-scheduler%E5%88%9D%E5%A7%8B%E5%8C%96.html</guid>
      <description>上篇文章结尾告诉我们了几个重要的全局变量，本篇文章会用到
go程序的启动 当我们运行一个main()函数来启动整个go程序的时候，他的启动流程是什么，这里我们从源码方面仔细分析下整个流程。
对于linux下的程序来说，入口文件在runtime/rt0_linux_amd64.s，
 具体我们是如何判断入口文件的，可以参考这篇文章，文章简单易懂
 跳转到runtime/asm_amd64.s文件的_rt0_amd64(SB)函数
跳转到runtime·rt0_go函数，我们保留该函数的主要流程
根据汇编代码，总结整个流程
 为g0，并分配栈空间 g0和m0相互绑定 // m.g0 = g0; g0.m = m0 osinit() OS初始化 schedinit() 调度器初始化 newproc() 将runtime.main作为参数创建goroutine mstart() 主线程进入调度循环  当初对tls的内容不了解，这里详细解释下
 set_tls()通过系统调用把m0.tls[1]的地址设置成了fs段的段基址。CPU中有个叫fs的段寄存器与之对应，而每个线程都有自己的一组CPU寄存器值，操作系统在把线程调离CPU运行时会帮我们把所有寄存器中的值保存在内存中，调度线程起来运行时又会从内存中把这些寄存器的值恢复到CPU。这样，在此之后，工作线程代码就可以通过fs寄存器来找到m.tls
 scheinit() 省略我们暂时不关心的代码
mcommoninit() ——初始化m0 主要用来初始化mp，包括设置id、设置random、设置gsignal、挂载到全局m中
procresize() ——初始化allp  procresize()通过GOMAXPROC可以动态的调整p的总个数，但其中涉及的问题比较复杂。
在这里我们只考虑初始化时的代码
  通过启动时候的schedinit调用procresize生成对应个数的P。因为可以通过runtime.GOMAXPROCS来动态修改P的个数，所以在procresize中会对P数组进行调整，或新增P或减少P。被减少的P会将自身的runable、runnext、gfree移到全局去。
 如果当前P不在多余的P中，则状态为running 如果当前P在多余的P中，则将当前M和P解绑，再将M和P数组的第一P绑定，并设为running 除了当前P外；所有P都设为idle，如果P中没有runnable,则将P加入全局空闲P,否则获取全局空闲M和P绑定。   References  golang 源码学习之GMP (goroutine) – 简书 开天辟地 —— Go scheduler 初始化（二） Go语言goroutine调度器初始化(12)  </description>
    </item>
    
    <item>
      <title>Go调度 | 1. GMP模型</title>
      <link>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-gmp%E6%A8%A1%E5%9E%8B.html</link>
      <pubDate>Wed, 30 Jun 2021 08:56:43 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/go%E8%B0%83%E5%BA%A6-gmp%E6%A8%A1%E5%9E%8B.html</guid>
      <description>本节深入先从宏观上介绍GMP模型，然后再通过源码进行深入了解。
线程和协程 协程是用户态的线程，他的管理和调度都是在用户态进行的，协程与线程相比有以下的优势：
 内存占用
线程内存占用约为4MB，且大小固定。协程内存占用约为2KB，并且是弹性可拓展的。
线程的内存分配往往一方面会造成内存的浪费，另一方面也会有栈溢出的风险。 创建和切换
线程的创建和切换都需要在内核态进行，并且线程切换时有很多寄存器需要进行现场保护。
而协程的创建和切换都是用户态的，并且线程保护的内容相对较少  GMP模型  G: goroutine协程,即我们定义的 go func(){}
P: processor处理器，是一种抽象的、用于G执行的局部资源
M: machine，也即thread，对应实际的内核线程
 GMP结构 这部分主要从源码层面分析GMP的结构
G结构 我们暂时忽略一些对我们分析没有影响的字段。
其中atomicstatus存储了goroutine的状态，这里列举一些重要的状态
 _Gidle: 刚刚被分配并且还没有被初始化 _Grunnable: 没有执行代码，没有栈的所有权，存储在运行队列中 _Grunning: 可以执行代码，拥有栈的所有权，被赋予了内核线程 M 和处理器 P _Gsyscall: 正在执行系统调用，拥有栈的所有权，没有执行用户代码，被赋予了内核线程 但是不在运行队列上 _Gwaiting: 由于运行时而被阻塞，没有执行用户代码并且不在运行队列上，但是可能存在Channel 的等待队列上 _Gdead: 没有被使用，没有执行代码，可能有分配的栈 _Gcopystack: 栈正在被拷贝，没有执行代码，不在运行队列上 _Gpreempted: 由于抢占而被阻塞，没有执行用户代码并且不在运行队列上，等待唤醒 _Gscan: GC 正在扫描栈空间，没有执行代码，可以与其他状态同时存在  M 结构 M代表操作系统的内核线程，他最多能创建10000个，但最多只有GOMAXPROCS个线程同时执行。如果不显式的指定，GOMAXPROCS默认为cpu的核心数。
golang使用runtime.m来表示操作系统线程。
 g0 是一个运行时中比较特殊的 Goroutine，它会深度参与运行时的调度过程，包括 Goroutine 的创建、大内存分配和 CGO 函数的执行。在后面的小节中，我们会经常看到 g0 的身影。
M 其实就是 OS 线程，它只有两个状态：自旋、非自旋</description>
    </item>
    
    <item>
      <title>Go | SliceTricks</title>
      <link>https://zhangyh.me/topics/golang/%E3%80%90%E8%AF%91%E3%80%91slicetricks.html</link>
      <pubDate>Tue, 29 Jun 2021 03:57:10 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/golang/%E3%80%90%E8%AF%91%E3%80%91slicetricks.html</guid>
      <description>原文地址: https://github.com/golang/go/wiki/SliceTricks
 go官方给出了许多slice操作的tricks，这里记录一下，并在最后给出Examples
AppendVector Copy Cut Delete Delete without preserving order  Cut
  Delete
  Delete without preserving order
 Expand Extend Filter(in place) Insert InsertVector Push Pop Push Front/Unshift Pop Front/Shift Additional Tricks Filtering without allocating slice使用相同的底层数组，因此可以重用该数组来进行filter操作。当然，原始的内容会被修改。
对于必须进行垃圾回收的元素，可以在上述代码后添加以下代码，进行元素的垃圾回收。
Reversing 也可以使用以下方式。（个人比较喜欢以下这种，更直观）
Shuffling Fisher–Yates 算法
 从go 1.10，可以使用 math/rand.Shuffle
 Batching with minimal allocation 如果想在一个超大的slice做批处理，这是很有用的
In-place deduplicate (comparable)  in-place 去重
 Move to front, or prepend if not present, in place if possible.</description>
    </item>
    
    <item>
      <title>《人民币汇率与人民币国际化》——翟东升</title>
      <link>https://zhangyh.me/topics/thinking/%E4%BA%BA%E6%B0%91%E5%B8%81%E6%B1%87%E7%8E%87%E4%B8%8E%E4%BA%BA%E6%B0%91%E5%B8%81%E5%9B%BD%E9%99%85%E5%8C%96-%E7%BF%9F%E4%B8%9C%E5%8D%87.html</link>
      <pubDate>Sun, 27 Jun 2021 06:18:43 +0000</pubDate>
      
      <guid>https://zhangyh.me/topics/thinking/%E4%BA%BA%E6%B0%91%E5%B8%81%E6%B1%87%E7%8E%87%E4%B8%8E%E4%BA%BA%E6%B0%91%E5%B8%81%E5%9B%BD%E9%99%85%E5%8C%96-%E7%BF%9F%E4%B8%9C%E5%8D%87.html</guid>
      <description>《人民币汇率与人民币国际化》-翟东升 总结翟东升老师的《人民币汇率与人民币国际化》基础课所讲的内容和知识点
1. 对人民币汇率的错误预测 看空人民币的五个错误理由 看空人民的观点错在哪 人民币是强势货币 2. 汇率的影响因素  短期看市场情绪，中期看政府调控，长期看市场
 影响长期汇率的直接因素 长期来看影响汇率最直接最有效的因素就是一个国家可贸易品的价格水平。可贸易品的加权平均价决定了汇率。
 两国之间的可贸易品价格如果差距太大，就会进行倒买倒卖，最终实现抛补平衡
 技术水平 和 人口老龄化 决定了可贸易品的价格，从而影响了汇率 (参考日本)
影响汇率的六个深层因素   国家能力 政府财政开支占国家GDP的比例越高，汇率往往越坚挺。
政府需要提供各种有效的公共产品(医疗、市场、教育、基础设施、贸易协定、治安秩序、商业秩序)。各种企业的成功离不开政府提供的公共产品。
  贸易开放度 出口所占比例。
   凡是想做空日元的都没有什么好下场
  要素特征 出口贸易品类型。 出口以能源、原材料、大宗商品、矿石等为主，汇率呈顺周期。 出口制成品为主，汇率呈逆周期。(汇率下跌，出口工业制成品获得价格优势，贸易顺差扩大，汇率获得上涨动能)
  文明类型 汇率最坚挺的两大文明：新教文明和东亚文明 鼓励生产，不鼓励消费
   新教文明：西欧、北欧、北美洲、大洋洲 东亚文明：日本、朝鲜半岛、中国大陆、中国台湾、中国香港、新加坡、越南
 比较软的货币所属文明：小乘佛教 不鼓励生产，不鼓励消费
贬值很厉害的货币所属文明：罗马天主教、东正教和伊斯兰教 不鼓励生产，鼓励消费
人均智商 宗教严肃度  海外投资的汇率风险 中国企业和资本出海需要重点防范：目标国的汇率风险 (结合上面所讲的因素进行思考)</description>
    </item>
    
  </channel>
</rss>
