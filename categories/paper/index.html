<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Paper | yuler's blog</title>
<meta name=keywords content>
<meta name=description content="yuler's blog">
<meta name=author content="yuler">
<link rel=canonical href=https://zhangyh.me/categories/paper/>
<meta name=google-site-verification content="G-JP3WQ36T5K">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabcsadf">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<link rel=icon href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=16x16 href=https://zhangyh.me/icon/jiaran16.ico>
<link rel=icon type=image/png sizes=32x32 href=https://zhangyh.me/icon/jiaran32.ico>
<link rel=apple-touch-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://zhangyh.me/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<link rel=alternate type=application/rss+xml href=https://zhangyh.me/categories/paper/index.xml>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JP3WQ36T5K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JP3WQ36T5K',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Paper">
<meta property="og:description" content="yuler's blog">
<meta property="og:type" content="website">
<meta property="og:url" content="https://zhangyh.me/categories/paper/"><meta property="og:image" content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="yuler's blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://zhangyh.me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Paper">
<meta name=twitter:description content="yuler's blog">
</head>
<body class=list id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://zhangyh.me/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://zhangyh.me/archives/ title=archives>
<span>archives</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/categories/ title=categories>
<span>categories</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://zhangyh.me/search/ title="search (Alt + /)" accesskey=/>
<span>search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<header class=page-header><div class=breadcrumbs><a href=https://zhangyh.me/>Home</a>&nbsp;»&nbsp;<a href=https://zhangyh.me/categories/>Categories</a></div>
<h1>Paper</h1>
</header>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>Reducing DRAM Footprint with NVM in Facebook
</h2>
</header>
<section class=entry-content>
<p>BACKGROUND RocksDB and MyRocks RocksDB是基于LSM的kv存储数据库,MyRocks在RocksDB基础上做了MySQL兼容
LinkBench 数据来自于Facebook真实生产环境下知识图谱中的数据.用于访问数据库的操作包括点读取、范围查询（例如通向节点的边列表）、计数查询以及简单的创建、删除和更新操作。
NVM NVM 是一种新的持久存储技术，有可能在数据中心和消费者用例中取代 DRAM. 既可作为字节寻址设备,也可以作为块设备.
INTUITION: THE CHALLENGE OF REPLACING DRAM WITH NVM 按字节计算,NVM比DRAM廉价,但NVM比Flash快一个数量级,因此很有希望作为第二层的存储. 然而NVM作为DRAM的替代品还有一些挑战: 高延迟,低带宽,耐久性,中断延迟
对于高延迟,可以用大容量NVM替代少容量DRAM作为性能补充(本实验用140G的NVM替换80G的内存),大容量的NVM可以容纳更多的cache,提高命中率. 对于低带宽,DRAM的带宽大于是NVM的35倍左右.我们使用NVM作为块设备,块大小为4kb,因此每次读数据只能以4kb的粒度去读.还有一种写数据的情况,少量的数据跨block存储,这会进一步消耗都带宽.因此只是kv存储的重要限制因素 对于耐久性,DRAM的耐磨损的,而NVM的写入次数是有限的.当NVM替换DRAM作为cache时,这种场景下,频繁的写入逐出会导致NVM的快速磨损.因此需要限制写入的次数. 对于中断延迟,由于相对于其他的块设备,NVM的延迟较低,中断的延迟就变得十分重要.(每次读延迟$10µs$,其中中断占了$2µs$.) 在这些基础上,设计了MyNVM.
Design Satisfying NVM’s Read Bandwidth MyNVM默认块大小为16kb,但LinkBench中的数据通常是几十几百字节,因此每次读一个对象,就需要读出16kb的data block,多读了160x左右的数据
从Fig.9可以看出需要两倍的带宽.
Reduce Block Size 直观上来看,减少block size是一个很直接的解决方案. 然而从Fig.9中可以看出从16kb减小到4kb,带宽消耗甚至增加了一倍. 问题就在于data block size减小四倍,就导致了index block size增加了四倍, 从Fig.9就可以看出. 加载到DRAM中的index block占据更多的空间,就导致了DRAM cache命中率降低,
改进方案就是,不再像之前的每个SSTable一个index block,而是使用多个index block,同时为index block创建一个top-level index block.
每次读先查缓存到DRAM中的top-level index block获取对应的index block,然后再把该index block缓存到DRAM中,从而减小了带宽消耗,用额外一次的查询来减少带宽以及内存空间占用.
然而目前带宽消耗还是太高.
Aligning Blocks With Physical Pages 对于$L_1$以及更底层的数据,会进行压缩....</p>
</section>
<footer class=entry-footer>10-18&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to Reducing DRAM Footprint with NVM in Facebook" href=https://zhangyh.me/posts/paper/oct/mynvm/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>TRIAD: Creating Synergies Between Memory, Disk and Log in Log Structured Key-Value Stores
</h2>
</header>
<section class=entry-content>
<p>Introduction 本文通过解决LSM-Tree的写放大问题,从而整体上提高其吞吐量. 在真是的生产环境中,作者观察到compaction最高可占据45%的CPU,平均每天有2.5个小时的时间花在compaction上. 显然,compaction和flush是重要的性能挑战,即使它们发生在面向用户的操作的关键路径之外.
主要面向
Background LSM-Tree:
Memory Component Disk Component Commit Log User-facing Operation:
Get(k) Update(k, v) Delete(k) Internal Operation:
Flushing Compaction Movitation 通过实验将正常的LSM与去掉了Disk Component的LSM相比,发现有3x的吞吐损失. 实验确定了昂贵的 I/O 操作的三个主要来源,每个来源对应于LSM-Tree中三个组件中的一个,即
data-skew unawareness (memory component) premature and iterative compaction (disk component) duplicated writes (commit log) data-skew unawareness hot key被频繁更新,造成了commit log快速增长,但$C_m$变化不大. 在$C_m$达到最大容量之前,频繁的触发更新（为什么？）,在L0中打开和存储文件的固定成本不会因实际向其中写入数据而摊销 data-skew导致$C_d$的多层都会有key的冗余,存储了大量的过时数据 premature and iterative compaction Leveling compaction和Size-Tiered Compaction 每次compaciton选择L0中的一个,但L0层的SSTable的key之间有交叉,即使两个SSTable的key交叉范围很大,仍需要进行两次Compaction,data-skew更是会恶化这种情况.
duplicated writes 当$C_m$刷新到$C_d$之后,需要删除掉其在Commit log中的部分....</p>
</section>
<footer class=entry-footer>10-12&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to TRIAD: Creating Synergies Between Memory, Disk and Log in Log Structured Key-Value Stores" href=https://zhangyh.me/posts/paper/oct/triad/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>FloDB: Unlocking Memory in Persistent Key-Value Stores
</h2>
</header>
<section class=entry-content>
<p>Shortcomings of current LSMs 线程的可扩展性 LevelDB允许一定程度的并发写，但内部是将写入操作写到了一个队列中，最后仍是顺序执行。 HyperLevelDB、RocksDB、cLSM做了一定程度的并行优化
内存的可扩展性 内存中组件可以是有序跳表，也可以是无序哈希表，但二者都不适合应用到更大的内存中。 对于跳表而言，读写时间复杂度都是$O(logn)$，有序性一定程度优化了范围查询，并加速了flush过程。但一般来说，读取操作都要到达磁盘，因此，跳表对写入的负面影响大于对读取的正面影响。对于更大内存中更大的跳表，读写延迟往往会提高。 对于哈希表而言，读写时间复杂度都是$O(1)$，无序性增加了范围查询和flush过程的工作量。但哈希表需要更长的时间排序，size越大，所需时间越长，会阻塞上层的写入操作。
FloDB Design 设计了两层的内存组件，上层是small、fast、unordered的Membuffer，下层是larger、ordered的Memtable。两层都是可并行的数据结构，数据流跟LSM类似，从最小的Membuffer，流向Memtable，直到磁盘上的SStable。
Get 搜索顺序：Membuffer -> Memtable -> SStable
Update Put和Delete操作和Update是相似的。 首先尝试在Membuffer更新，如果Membuffer满了，就直接在Memtable上完成更新。（假如说Membuffer或Memtable上有对应值的话，更新就是in-place的）
替代的方案就是mulit-version更新，然而对比起来，in-place更新在skewed的工作负载上具有更好的表现。
Scan 扫描Memtable和disk，同时允许Membuffer进行并行的update。 挑战1：Membuffer有对应的数据，因此扫描前先清空Membuffer到Memtable上。 挑战2：long-time的扫描导致Membuffer变满，允许writer和sanner在Memtable上并行。这个时候可能会导致不一致性，因此我们记录scan时的sequence number，扫描后的结果中如果有大于该值的entry，则重新扫描。
FloDB Implementation 明确了四各方面:
Membuffer和Memtable数据结构的选择 数据从Membuffer到Memtable移动的机制 新颖的多插入操作用于简化 Memtable 和 Membuffer 之间的数据流 面向用户的操作的实现 Memory Component Implementation Membuffer -> Hash Table Memtable -> Skiplist
Interaction Between Levels 包括persisting和draining.
Skiplist Multi-inserts 实验显示，多线程下hash table比skiplist块一到两个数量级。因此应该尽快地完成移动，使得update在hash table上进行。
批量的插入多个kv对到skiplist中，而不是挨个插入。核心思想是我们能直接利用前一个kv对的pre指针，而不是重新找，有path-reuse的思想在里面。
并行性。插入和insert、read并行。
key的临近度决定了path-reuse的效果。</p>
</section>
<footer class=entry-footer>9-26&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to FloDB: Unlocking Memory in Persistent Key-Value Stores" href=https://zhangyh.me/posts/paper/sep/flodb/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores
</h2>
</header>
<section class=entry-content>
<p>Movitation 传统的merge policy Level Merge Policy和Tiered Merge Policy在读写放大方面各有优势. 前者相当于单层的Tiered,保证了单层数据的有序性,减少了读放大,但要维护这种有序性,就需要频繁的merge,因此造成了严重的写放大. 后者对有序性的要求并不严格,它只要求每个group的key range互不交叉,允许sstable之间的key交叉.
工作负载的空间局部性 作者对ZippyDB(facebook)的工作负载分析发现,少部分的key被读写多次,大部分的key被读写的次数很少.
Design 作者提出了疑问:
How to resolve the read amplification problem induced by the tiering merge policy while maintaining its low write amplification?
根据key-space的不同access pattern,有选择的采用Tiering和Leveling Merge policy. 通过追踪每层中的key-space的访问强度,来决定设置T的值为1还是configured threshold.
Spatially Fragmented LSM-TREE (SFM) 每层通过随机的边界分为不同的key-space.边界的选取方式类似于PebblesDB. 如果一个栈的key-range被认为是读密集的,则设置T为1,否则,按照原有的配置设置.
合并策略如上图
读超过stack阈值的文件到内存 在内存中进行merge 按照下层的key边界进行切片 新SSTable被刷新到下层 添加到key对应的栈 B. Read Intensity Identification 如何判断读的强度呢? 统计每个段的读写次数.对于读操作,即使读取的key不存在,也是要计入读取次数的. 对于写操作,统计每次merge后的sstable中的kv对的数量. 通过对比每个stack的r/w和整体平均的r/w来判断其为hot还是cold. 根据每个stack的hot还是cold属性,来决定key-space的访问强度.
Metadata Managment 通过StackMetadata的数据结构来管理每个stack的元信息....</p>
</section>
<footer class=entry-footer>9-24&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to SFM: Mitigating Read/Write Amplification Problem of LSM-Tree-Based Key-Value Stores" href=https://zhangyh.me/posts/paper/sep/sfm/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores
</h2>
</header>
<section class=entry-content>
<p>本文旨在解决LSM-Tree的长尾问题，长尾问题的根源主要是客户端操作和LSM-Tree内部操作的互相干扰。 客户端操作：write 内部操作: flush、compaction
Experiment study of tail latency 实验一：对比RocksDB和RocksDB without flushing. RocksDB without flushing: 要flush的immutable memtable直接丢弃 实验二：Rate-limited RocksDB 实验三：RocksDB with increased Memtable 实验四：TRAID 实验五：PebblesDB 作者对目前最新的LSM-Tree数据库RocksDB、TRIAD、PebblesDB进行了实验并得出三点结论。
长尾的主要原因是被写满的memtable阻塞了write。 有两个原因导致了这种情况：第一，磁盘上的L0-L1 compaction跟不上写入的速度，导致L0被写满。第二，意外的有大量的compaction在同时进行，占据了大量的IO，导致flush因为有限的带宽而变慢，使得memtable变满。 简单的限制内部操作带宽, 并不能解决flush带宽受限的问题，长远来看反而会加剧这种问题。这种方法能够推迟压缩，但增加了在未来某个时候同时发生compaction的可能性。 提高吞吐量的方法，例如选择性地启动压缩或仅在最高level执行压缩，在短期内避免了延迟峰值，但从长远来看会加剧问题，因为它们也会增加在稍后的某个时间点进行许多并发压缩的可能性。 推论： 由1得出推论：内部操作并不是完全平等的。更low-level的操作是关键的，因为未能及时完成它们可能会导致客户端操作停滞。 由2，3得出推论：必须进行长时间的测试，避免问题未被发现。 (因为写入的数据量不够的话，无法体现更底层的compaction操作对系统的影响。)
SILK Design principles 有选择的分配带宽。 遇到峰值流量时，分配更多的带宽给low-level，减缓high-level的压缩。 在闲时，利用短暂的低负载时期来促进内部操作的处理 优先处理更low-level的操作 给内部操作指定优先级: flush > L0-L1 compaction > high-level compaction 抢占式compaction 允许更low-level的compaction抢占high-level的compaction Implementation 有选择的分配带宽 有优先次序和可抢占的内部操作 系统内部维护两个线程池：高优先级池用于flush,低优先级池用于compaction. Flush: 有专门的线程池，在mem被写满之前，提供稳定的一定量的带宽，以保证不断的写入。多个内存组件和多个线程可能保证较长的性能高峰。 L0-L1 compaction: 和high-level的compaction共用线程，该过程需要保证L0有足够的空间供flush。如果需要进行L0-L1的compaction，并且目前没有可用线程，那么就会抢占high-leve compaction的线程。 high-level compaction: 线程维护在低优先级线程池 </p>
</section>
<footer class=entry-footer>9-22&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores " href=https://zhangyh.me/posts/paper/sep/silk/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>The Google File System
</h2>
</header>
<section class=entry-content>
<p>1 Introduction 本文是《The Google File System》的学习笔记
2 Design Overview 2.1 Assumptions GFS基于以下基本假设作为设计原则。
廉价商用机器出故障是常态 主要存储大文件，通常大于100MB，没有针对小文件的优化 读操作：大规模的流式读、小规模的随机读 写操作：大规模的append写操作、小规模的随机写效率低 支持多个客户端并行 append 到同一个文件 持续的高带宽比低时延重要 2.2 Interface 未实现POSIX API 支持常见文件操作，包括create, delete, open, close, read, write. 包括 snapshot 和 record append 2.3 Architecture 集群通常包括一个 master、多个 chunkserver 和多个访问的 client 文件使用固定大小(64 MB)的 chunk 存储，使用globally unique、immutable的 64bit chunk handle表示 master 维护整个集群所有的 metadata ，通过 heartbeat 与 chunkserve 进行 instruct、state-collect client 以 lib 的形式嵌入到应用中，以供 application 调用 client 和 chunkserver 都不 cache file data，但 client 会 cache metadata 2....</p>
</section>
<footer class=entry-footer>8-28&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to The Google File System" href=https://zhangyh.me/posts/paper/aug/gfs/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>LSM-based storage techniques: a survey
</h2>
</header>
<section class=entry-content>
<p> paper原文：LSM-based storage techniques: a survey
LSM-tree basics 对于索引结构通常有两种更新策略，即in-place update和out-of-place update。
in-place通常使用B+树作为底层数据结构，通过随机写来更新数据项，带来的是读数据的优化，同时每个数据项只存有一份，因此节约了空间。
out-place通常使用lsm tree作为底层数据结构，将随机写转化为顺序写，加快了写的速度，但同时降低了读数据的性能，另外，对同一个key，存有多个版本，带来了空间上的浪费。
LevelDB是谷歌开源的kv数据库，它基于内存-磁盘的存储层次，实现了LSM Tree最基本的功能。 内存中有memtable，使用skiplist来按顺序存储kv对 磁盘中有分层的sstable，使用sorted-string table以文件的形式来存储。 sstable包括了data block、index block、footer block
如今的lsm tree通常为了加快读速度，往往会具有bloom filter组件，该组件有两个操作：插入key和检查key是否存在。为磁盘中的每一level，维护一个bloom filter，从而提高读性能。需要注意，它是false positive的。 同时因为该结构比较小，往往把他缓存到内存当中。
对于failure recovery，使用WAL保证内存中memtable的recovery，使用manifest保证merge过程中sstable的recovery。
有两种常用的merge policy，如上图所示。 Leveling merge policy，要求每一层的SSTable之间不能overlap，merge的时候将第i层的SSTable和第i+1层overlape的所有SSTables进行merge，然后写入i+1层。也就是说通过leveling通过频繁的merge，使得每一层全部有序，牺牲部分写性能来换取读性能。
Tiering merge policy，要求每一层的SSTable之间可以overlap，在sstable达到一定的数量或者size要求后，对该层所有的SSTable进行merge sort，然后直接flush进下一层。该策略通过减少merge操作，牺牲部分读性能换取了写性能。
LSM-tree improvement 文章将对LSM-tree的优化分为以下几个方面：
Write Amplification: out-of-place数据结构所具有的问题，降低了写性能和磁盘寿命。 Merge Operation: merge后会造成buffer cache miss，大数据量的merge会造成write stall Hardware: 针对large memory,multi-core, SSD/NVM, native storage的优化 Special Workloads: Auto-tuning: 不可能同时达到read,write,space同时最优，并且由于lsm tree拥有大量参数，手动调优也是困难的。 Secondary Indexing: lsm tree只支持简单的kv接口，二级索引也是一个方向。 </p>
</section>
<footer class=entry-footer>8-10&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to LSM-based storage techniques: a survey" href=https://zhangyh.me/posts/paper/aug/suvery-of-lsm/></a>
</article>
<article class="post-entry tag-entry">
<header class=entry-header>
<h2>Wisckey: Separating keys from values in ssd-conscious storage
</h2>
</header>
<section class=entry-content>
<p>WiscKey Design Goals Low写放大 Low读放大 SSD优化 Feature-rich API (scan、snapshot) Realistic k-v size (value的size通常比key的大很多) K-V Separation LSM tree的主要性能成本在于Compaction，但它维护了key的有序性，对于加速读也是十分有必要的。 Compaction只需要对key排序，因此考虑将k-v分开存储。只把value的地址和key放在一起。 LSM tree的size将大大变小，减少了写放大。 读操作虽然需要一次额外的寻址操作，但更小的LSM tree加快了检索同时更容易缓存到大内存中。 Challenge Parallel Range Query 在levelDB中，范围查询通过iterator的顺序读实现，但是现在由于和key一同存储的只有value的地址，范围查询变为了随机io。 我们在读取所有的地址后，写入一个队列，通过多线程并发读取，来加快范围查询的性能。
Garbage Collection 由于把所有的数据都存到了vLog里，最朴素的垃圾回收的方法当然是扫描一遍LSM-tree，但开销太大了，只适用于离线环境。 WiscKey在vlog中存储(ksize, vsize, key, value)，并维护head和tail指针。 数据在head处添加 垃圾回收线程从tail处扫描，每次扫描一定量的数据（几MB），然后在LSM-tree中查找，如果kv对有效的话，则再把它添加到head处。 因此，只有tail和head之间的数据是有效的。
Crash Consistency Optimizations Value-Log Write Buffer 对于写密集的数据集，频繁的大量的小size的value写入，会导致较大的系统开销。 因此考虑在内存中维护Buffer。 当读数据的时候，先到vlog write buffer中查找，如果没有，再查vlog。 crash-consistency的维护类似于levelDB，使用WAL。
Optimizing the LSM-tree Log 在写LSM-tree之前需要先写vlog，因为这里我们加了vlog write buffer，所以我们直接用vlog的日志作为WAL。</p>
</section>
<footer class=entry-footer>8-10&nbsp;·&nbsp;yuler</footer>
<a class=entry-link aria-label="post link to Wisckey: Separating keys from values in ssd-conscious storage" href=https://zhangyh.me/posts/paper/aug/wisckey/></a>
</article>
<footer class=page-footer>
<nav class=pagination>
<a class=next href=https://zhangyh.me/categories/paper/page/2/>Next Page »</a>
</nav>
</footer>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://zhangyh.me/>yuler's blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>